{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sony Lifelog: Data science of sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jesper Bitsch, s113730@student.dtu.dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for converting and cleaning a part of the Sony Lifelog dataset. The notebook should be read in conjuction with the thesis.\n",
    "\n",
    "Links:\n",
    "<ul>\n",
    "<li><a href=\"http://spark.apache.org/docs/2.0.0/api/python/_modules/pyspark/sql/functions.html\">PySpark SQL functions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import operator\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "from collections import Counter\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yarn application --list\n",
    "#yarn application -kill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "#from pyspark.sql import SQLContext, Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, DateType, BooleanType, IntegerType, TimestampType, DoubleType, FloatType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = \"\"\n",
    "bucket_path = \"\"\n",
    "timebin_interval = 5 #Change to what ever timeinterval we want (In minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start and end dates for the selected months\n",
    "start_end_date_times_for_periods = [\n",
    "    (datetime.datetime(2015, 10, 1,0,0,0),datetime.datetime(2015, 10, 31, 23, 59, 0)),\n",
    "    (datetime.datetime(2015, 11, 1,0,0,0),datetime.datetime(2015, 11, 30, 23, 59, 0)),\n",
    "    (datetime.datetime(2015, 12, 1,0,0,0),datetime.datetime(2015, 12, 31, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 1, 1,0,0,0),datetime.datetime(2016, 1, 31, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 2, 1,0,0,0),datetime.datetime(2016, 2, 29, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 3, 1,0,0,0),datetime.datetime(2016, 3, 31, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 4, 1,0,0,0),datetime.datetime(2016, 4, 30, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 5, 1,0,0,0),datetime.datetime(2016, 5, 31, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 6, 1,0,0,0),datetime.datetime(2016, 6, 30, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 7, 1,0,0,0),datetime.datetime(2016, 7, 31, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 8, 1,0,0,0),datetime.datetime(2016, 8, 31, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 9, 1,0,0,0),datetime.datetime(2016, 9, 30, 23, 59, 0)),\n",
    "    (datetime.datetime(2016, 10, 1,0,0,0),datetime.datetime(2016, 10, 31, 23, 59, 0))\n",
    "]\n",
    "\n",
    "#Folders which is used for the study\n",
    "folders_to_load = [\n",
    "    \"yearmonth=201510\", \"yearmonth=201511\", \"yearmonth=201512\", \"yearmonth=201601\", \"yearmonth=201602\", \n",
    "    \"yearmonth=201603\", \"yearmonth=201604\", \"yearmonth=201605\", \"yearmonth=201606\", \"yearmonth=201607\",\n",
    "    \"yearmonth=201608\", \"yearmonth=201609\", \"yearmonth=201610\"\n",
    "]\n",
    "\n",
    "#start and end datetimes for the selected months. \n",
    "start_end_times_for_periods = [\n",
    "    (datetime.date(2015, 10, 1),datetime.date(2015, 10, 31)),\n",
    "    (datetime.date(2015, 11, 1),datetime.date(2015, 11, 30)),\n",
    "    (datetime.date(2015, 12, 1),datetime.date(2015, 12, 31)),\n",
    "    (datetime.date(2016, 1, 1),datetime.date(2016, 1, 31)),\n",
    "    (datetime.date(2016, 2, 1),datetime.date(2016, 2, 29)),\n",
    "    (datetime.date(2016, 3, 1),datetime.date(2016, 3, 31)),\n",
    "    (datetime.date(2016, 4, 1),datetime.date(2016, 4, 30)),\n",
    "    (datetime.date(2016, 5, 1),datetime.date(2016, 5, 31)),\n",
    "    (datetime.date(2016, 6, 1),datetime.date(2016, 6, 30)),\n",
    "    (datetime.date(2016, 7, 1),datetime.date(2016, 7, 31)),\n",
    "    (datetime.date(2016, 8, 1),datetime.date(2016, 8, 31)),\n",
    "    (datetime.date(2016, 9, 1),datetime.date(2016, 9, 30)),\n",
    "    (datetime.date(2016, 10, 1),datetime.date(2016, 10, 31))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format the sleep dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the total count and what influence it has when removing all SWR10 and users with less than 10 registered sleep entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = []\n",
    "for folder in folders_to_load:\n",
    "    arr.append(pre_path+\"sleepactivity/\"+folder+\"/*.avro\")\n",
    "    \n",
    "df = (sqlContext.read.format(\"com.databricks.spark.avro\").load(arr))\n",
    "\n",
    "un_all = df.select(\"useruuid\").distinct().collect()\n",
    "un_all_limit = df.select(\"useruuid\").groupBy(\"useruuid\").count().where(\"count > 9\").collect()  \n",
    "\n",
    "df_no_swr10 = df.withColumn('device_name', func_get_device_type_udf(F.col('devices'))).where(\"device_name != 'swr10'\")\\\n",
    "        .select(\"useruuid\")\n",
    "    \n",
    "un_no_swr10 = df_no_swr10.distinct().collect()\n",
    "un_no_swr10_limit = select(\"useruuid\").groupBy(\"useruuid\").count().where(\"count > 9\").collect()\n",
    "\n",
    "\n",
    "print len(un_all)\n",
    "print len(un_all_limit)\n",
    "print len(un_no_swr10)\n",
    "print len(un_no_swr10_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can format and remove the entries we dont want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, folder in enumerate(folders_to_load):\n",
    "    print \"########### Starting with \", folder, \"###########\"\n",
    "    \n",
    "    #Defines the period, so we can remove items which has been added to late. \n",
    "    times_for_period = start_end_times_for_periods[i]\n",
    "    \n",
    "    #Load the dataset for the given period. \n",
    "    df_sleep_raw = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(pre_path+\"/sleepactivity/\"+folder+\"/*.avro\")).where(\"deleted_time == ''\")\\\n",
    "      .withColumn('device_name', func_get_device_type_udf(F.col('devices'))).where(\"device_name != 'swr10'\")\\\n",
    "      .select(\"*\", F.col(\"start_time\").cast(\"date\").alias(\"start_date\"), F.col(\"end_time\").cast(\"timestamp\").alias(\"lookup_time\"))\\\n",
    "      .where(F.col(\"start_date\") > times_for_period[0])\\\n",
    "      .where(F.col(\"start_date\") < times_for_period[1])\n",
    "    \n",
    "    print \"Data loaded\"\n",
    "    \n",
    "    #Make a dataframe with the newest enty for each of the id's \n",
    "    newest_ids = df_sleep_raw.select(F.col(\"id\").alias(\"dummy_id\"),\"lookup_time\")\\\n",
    "        .groupBy(F.col(\"dummy_id\"))\\\n",
    "        .agg(F.max(F.col(\"lookup_time\")))\n",
    "        \n",
    "    print \"Newest entries found\"\n",
    "    \n",
    "    #join the two dataframes\n",
    "    df_sleep_no_doublets = df_sleep_raw.join(newest_ids, \n",
    "                                        (df_sleep_raw[\"lookup_time\"] == newest_ids[\"max(lookup_time)\"]) & \n",
    "                                        (df_sleep_raw[\"id\"] == newest_ids[\"dummy_id\"]))\n",
    "    \n",
    "    print \"Removed duplicates\"\n",
    "    \n",
    "    #Only take the relevant columns. \n",
    "    df_sleep_clean = df_sleep_no_doublets.select(F.col(\"id\"),\n",
    "                                                 F.col(\"useruuid\"),\n",
    "                                                 F.col(\"start_time\"),\n",
    "                                                 F.col(\"end_time\"),\n",
    "                                                 F.col(\"device_name\"),\n",
    "                                                 F.col(\"sleep_state\"))\n",
    "    \n",
    "    #Merge sleep entries togheter (20 minutes). \n",
    "    rdd_sleep = df_sleep_clean.rdd\\\n",
    "        .map(lambda item: (item[\"useruuid\"],item))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(map_sleep_items)\\\n",
    "        .flatMap(lambda x : x[1])\n",
    "        \n",
    "    print \"Merging sleep entries\"\n",
    "\n",
    "    #converts to dataframe\n",
    "    df_sleep_sleep_merged = rdd_sleep.toDF()\n",
    "    \n",
    "    print \"Merged sleep entries\"\n",
    "    \n",
    "    #we want at least 10 entries from the user\n",
    "    df_sleep_sleep_good_ids = df_sleep_sleep_merged\\\n",
    "        .groupBy(F.col(\"useruuid\").alias(\"dummy_id\")).count()\\\n",
    "        .where(F.col(\"count\") >= 10)\n",
    "    \n",
    "    #Join the dataframes\n",
    "    df_sleep_final = df_sleep_sleep_merged.join(df_sleep_sleep_good_ids, \n",
    "                (df_sleep_sleep_merged[\"useruuid\"] == df_sleep_sleep_good_ids[\"dummy_id\"]))\\\n",
    "        .drop(\"dummy_id\").drop(\"count\")\n",
    "        \n",
    "    print \"Removed userers with less than then 10 entries\"\n",
    "        \n",
    "    df_sleep_final.write.format('com.databricks.spark.avro').save(bucket_path+\"sleep_\"+folder)\n",
    "    \n",
    "    print \"Dataset saved to s3\"\n",
    "\n",
    "    print \"Done with folder \", folder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chronotype, social jet lag etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('useruuid', StringType(), True),\n",
    "                     StructField('avg_sleep_state', FloatType(), True),\n",
    "                     StructField('avg_start_work', FloatType(), True),\n",
    "                     StructField('avg_start_free', FloatType(), True),\n",
    "                     StructField('avg_end_work', FloatType(), True),\n",
    "                     StructField('avg_end_free', FloatType(), True),\n",
    "                     StructField('msw', FloatType(), True),\n",
    "                     StructField('msf', FloatType(), True),\n",
    "                     StructField('msf_c', FloatType(), True),\n",
    "                     StructField('msf_c_month', FloatType(), True),\n",
    "                     StructField('sj_month', FloatType(), True),\n",
    "                     StructField('sj', FloatType(), True),\n",
    "                     StructField('std_start_work', FloatType(), True),\n",
    "                     StructField('std_end_work', FloatType(), True),\n",
    "                     StructField('std_start_free', FloatType(), True),\n",
    "                     StructField('std_end_free', FloatType(), True),\n",
    "                     StructField('std_start', FloatType(), True),\n",
    "                     StructField('std_end', FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for folder in folders_to_load:\n",
    "    df_period = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_\"+folder+\"/*.avro\")) \n",
    "    \n",
    "    print \"Loaded for\", folder\n",
    "    df_sleep_user_info = df_period.rdd\\\n",
    "        .map(lambda item: (item[\"useruuid\"],item))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(map_users_sleep_items_to_info)\\\n",
    "        .map(lambda (key,value) : value).toDF(schema)\n",
    "    \n",
    "    print \"Mapped info for\", folder\n",
    "    df_sleep_user_info.write.format('com.databricks.spark.avro').save(bucket_path+\"sleep_stats_\"+folder)\n",
    "    print \"Info set saved to s3\", folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all_files_arr = []\n",
    "for folder in folders_to_load: \n",
    "    load_all_files_arr.append(bucket_path+\"sleep_\"+folder+\"/*.avro\")\n",
    "    \n",
    "df_total = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(load_all_files_arr)) \n",
    "            \n",
    "df_sleep_user_info = df_total.rdd\\\n",
    "        .map(lambda item: (item[\"useruuid\"],item))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(map_users_sleep_items_to_info)\\\n",
    "        .map(lambda (key,value) : value).toDF(schema)\n",
    "\n",
    "df_sleep_user_info.write.format('com.databricks.spark.avro').save(bucket_path+\"sleep_stats_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sleep_items(items):\n",
    "    import math, decimal, datetime\n",
    "    import numpy as np\n",
    "    from dateutil.parser import parse\n",
    "\n",
    "    def data_func_from_string(d):\n",
    "        date = parse(d)\n",
    "        unaware_date = date.replace(tzinfo=None)\n",
    "    \n",
    "        return unaware_date\n",
    "    \n",
    "    items = list(items)\n",
    "    items.sort(key=lambda item:data_func_from_string(item['start_time']), reverse=False)\n",
    "    \n",
    "    sleep_items = []\n",
    "    #Loop through sub \n",
    "    sleepItemsToUpdate = {}\n",
    "    sleepItemsToDelete = []\n",
    "    delta_merge = datetime.timedelta(minutes=25) #The interval which is allowed between two sleep entries. \n",
    "\n",
    "    #Find sleeps to merge\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        for j in range(i+1,len(items)):\n",
    "            sub_item = items[j].asDict()\n",
    "            if item[\"id\"] != sub_item[\"id\"] and \\\n",
    "                data_func_from_string(item[\"end_time\"]) < data_func_from_string(sub_item[\"start_time\"]) and \\\n",
    "                data_func_from_string(item[\"end_time\"]) > data_func_from_string(sub_item[\"start_time\"]) - delta_merge:\n",
    "                    sleepItemsToDelete.append(sub_item[\"id\"])\n",
    "                    sleepItemsToUpdate[item[\"id\"]]  = {\n",
    "                            \"id2\" : sub_item[\"id\"],\n",
    "                            \"new_endtime_dt\" : data_func_from_string(sub_item[\"end_time\"]),\n",
    "                            \"new_endtime_str\" : sub_item[\"end_time\"],\n",
    "                            \"new_states\" : sub_item[\"sleep_state\"],\n",
    "                            \"start_time_two\" : data_func_from_string(sub_item[\"start_time\"])\n",
    "                            \n",
    "                        } \n",
    "            else:\n",
    "                #No sleeps close enough to merge. We know that because they are sorted. \n",
    "                break\n",
    "\n",
    "    #Loop through all items to generates new sleep entries. \n",
    "    for i in range(len(items)):\n",
    "        num_awake = 0 #Times awake during a night\n",
    "        awake_time = float(0) #Total time in seconds a user is awake during a night. \n",
    "        item = items[i].asDict()\n",
    "        item[\"end_time_dt\"] = data_func_from_string(item[\"end_time\"])\n",
    "        item[\"start_time_dt\"] = data_func_from_string(item[\"start_time\"])\n",
    "            \n",
    "        #The item is a subitem to another, and we will therefore discard it. \n",
    "        if item[\"id\"] in sleepItemsToDelete:\n",
    "            continue\n",
    "            \n",
    "        newEndDate = item[\"end_time_dt\"]\n",
    "        newEndDateStr = item[\"end_time\"]\n",
    "        lookup_id = item[\"id\"]\n",
    "        #Find entries to which should be updated. \n",
    "        while True:\n",
    "            if lookup_id in sleepItemsToUpdate:\n",
    "                num_awake += 1\n",
    "                lookupItem = sleepItemsToUpdate[lookup_id]\n",
    "                start_time_two = lookupItem[\"start_time_two\"]\n",
    "                awake_time += (start_time_two-newEndDate).total_seconds()\n",
    "                newEndDate = lookupItem[\"new_endtime_dt\"]\n",
    "                newEndDateStr = lookupItem[\"new_endtime_str\"]\n",
    "                newStates = lookupItem[\"new_states\"]\n",
    "                item[\"sleep_state\"] += newStates\n",
    "            \n",
    "                lookup_id = lookupItem[\"id2\"]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        date = item[\"start_time_dt\"]\n",
    "        day = -1\n",
    "        if date.hour < 4:\n",
    "            day = date.weekday()\n",
    "            if day == 0:\n",
    "                day = 6\n",
    "            else:\n",
    "                 day -= 1\n",
    "        else:\n",
    "             day = date.weekday()\n",
    "    \n",
    "        item[\"weekday\"] = day\n",
    "    \n",
    "        def calculate_moon_phase(date):\n",
    "            \"\"\"Method which returns and int which defienes the moon phase for the given tiem.\"\"\"\n",
    "            dec = decimal.Decimal\n",
    "            now = date.replace(tzinfo=None)\n",
    "\n",
    "            diff = now - datetime.datetime(2001, 1, 1)\n",
    "            days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
    "            lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
    "    \n",
    "            pos = lunations % dec(1)\n",
    "    \n",
    "            index = (pos * dec(8)) + dec(\"0.5\")\n",
    "            index = math.floor(index)\n",
    "            \n",
    "            return int(index) & 7\n",
    "\n",
    "        def parse_utc(date):\n",
    "            \"\"\"Generates unaware UTC datetime from string\"\"\"\n",
    "            parse_date = parse(date)\n",
    "            date_utc = parse_date - parse_date.utcoffset()\n",
    "            unaware_date = date_utc.replace(tzinfo=None)\n",
    "            return unaware_date\n",
    "        \n",
    "        def genrate_key(dt,roundTo):\n",
    "            seconds = (dt - dt.min).seconds\n",
    "            rounding = (seconds+roundTo/2) // roundTo * roundTo\n",
    "            new_dt = dt + datetime.timedelta(0,rounding-seconds,-dt.microsecond)\n",
    "            \n",
    "            return str(new_dt.hour) + \"-\" + str(new_dt.minute)\n",
    "        \n",
    "        sleep_states = item[\"sleep_state\"]\n",
    "        deep_sleep_count = sleep_states.count(2)\n",
    "        \n",
    "        adjust_sleep_duration = float(0)\n",
    "        is_awake = False\n",
    "        is_start = True\n",
    "        adjust_start_seconds = 0 \n",
    "        for state in sleep_states:\n",
    "            if state == 0:\n",
    "                adjust_sleep_duration += float(60)\n",
    "                if is_awake == False:\n",
    "                    num_awake += 1\n",
    "                is_awake = True\n",
    "                if is_start == True:\n",
    "                    adjust_start_seconds += 60\n",
    "            else:\n",
    "                is_start = False\n",
    "                is_awake = False\n",
    "        \n",
    "        item[\"start_time_dt\"] = item[\"start_time_dt\"] + datetime.timedelta(seconds=adjust_start_seconds)\n",
    "        item[\"week_number_key_user\"] = item[\"useruuid\"]+\"_\"+str(item[\"start_time_dt\"].isocalendar()[1])\n",
    "        item[\"end_time_dt\"] = newEndDate\n",
    "        item[\"end_time\"] = newEndDateStr\n",
    "        item[\"adjusted_sleep_duration\"] = abs((newEndDate-item[\"start_time_dt\"]).total_seconds()) - abs(awake_time) - adjust_sleep_duration\n",
    "        item[\"awake_time\"] = awake_time\n",
    "        delta = (item[\"end_time_dt\"]-item[\"start_time_dt\"]).total_seconds()\n",
    "        item[\"sleep_duration\"] = delta\n",
    "        item[\"time_awake\"] = num_awake\n",
    "        \n",
    "        item[\"moon_phase\"] = calculate_moon_phase(date)\n",
    "        item[\"average_sleep\"] = float(np.mean(sleep_states))\n",
    "        item[\"start_time_dt_utc\"] = parse_utc(item[\"start_time\"]) + datetime.timedelta(seconds=adjust_start_seconds)\n",
    "        item[\"end_time_dt_utc\"] = parse_utc(item[\"end_time\"]) \n",
    "        item[\"start_key\"] = genrate_key(item[\"start_time_dt\"],60*15)\n",
    "        item[\"end_key\"] = genrate_key(item[\"end_time_dt\"],60*15)\n",
    "        \n",
    "        if deep_sleep_count > 0:\n",
    "            item[\"p_deep_sleep\"] = deep_sleep_count/float(len(sleep_states))\n",
    "            item[\"time_before_deep_sleep\"] = (sleep_states.index(2)+1)*60 #convert to seconds\n",
    "        else:\n",
    "            item[\"p_deep_sleep\"] = float(0)\n",
    "            item[\"time_before_deep_sleep\"] = float(-1.0)\n",
    "            \n",
    "        if item[\"sleep_duration\"] < 50400 and item[\"sleep_duration\"] > 10800:\n",
    "            sleep_items.append(item)\n",
    "            \n",
    "    return sleep_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_users_sleep_items_to_info(items):\n",
    "    \n",
    "    import math, decimal, datetime\n",
    "    import numpy as np\n",
    "    from dateutil.parser import parse\n",
    "    \n",
    "    def adjusted_msf(msf_str,duration_free,duration_work):\n",
    "        \"\"\"Functiosn which returns the adjusted MSF: Alogorithm is found in the litereature. See report.\"\"\"\n",
    "        adjusted_seconds = 0.5 * ( duration_free - (5*duration_work + 2*duration_free) /float(7))\n",
    "        delta = datetime.timedelta(seconds=adjusted_seconds)\n",
    "        msf = datetime.datetime(2015, 9, 1, msf_str[0], msf_str[1],msf_str[2])\n",
    "        msf -= delta\n",
    "        \n",
    "        return (msf.hour,msf.minute, msf.second)\n",
    "\n",
    "    \n",
    "    def get_midpoint(date_str,duration):\n",
    "        \"\"\"Get midpoint of datetime. Is done by adding half of the duration to the beginning\"\"\"\n",
    "        seconds_to_add = duration / 2\n",
    "        delta = datetime.timedelta(seconds=seconds_to_add)\n",
    "    \n",
    "        date = datetime.datetime(2015, 9, 1, date_str[0], date_str[1],date_str[2])\n",
    "        date += delta\n",
    "    \n",
    "        return (date.hour,date.minute, date.second)\n",
    "\n",
    "    def data_func_from_string(d):\n",
    "        \"\"\"Converts a date string to a unaware local datetime. Used to detect when people are going to sleep\"\"\"\n",
    "        from dateutil.parser import parse\n",
    "        date = parse(d)\n",
    "        unaware_date = date.replace(tzinfo=None)\n",
    "    \n",
    "        return unaware_date\n",
    "    \n",
    "    def get_average_timestamp(times,adjust):\n",
    "        \"\"\"\n",
    "        Function which returns the average time for a list of datetimes. If adjust == True, all dates with hour < 12 \n",
    "        will be added additionally 24 hours. \n",
    "        \"\"\"\n",
    "        times = [data_func_from_string(x) for x in times]\n",
    "        if adjust:\n",
    "            time_list = map(lambda s : s.second + 60*(s.minute + 60*(s.hour+24)) if s.hour < 12\n",
    "                else s.second + 60*(s.minute + 60*s.hour),\n",
    "                times)\n",
    "        else:\n",
    "            time_list = map(lambda s : s.second + 60*(s.minute + 60*s.hour),\n",
    "                times)\n",
    "            \n",
    "        average = sum(time_list)/len(time_list)\n",
    "        bigmins, secs = divmod(average, 60)\n",
    "        hours, mins = divmod(bigmins, 60)\n",
    "    \n",
    "        if hours >= 24:\n",
    "            hours = hours-24\n",
    "\n",
    "        return (hours, mins, secs)\n",
    "    \n",
    "    def get_std_timestamp(times,adjust):\n",
    "        times = [data_func_from_string(x) for x in times]\n",
    "        \n",
    "        if adjust:\n",
    "            time_list = map(lambda s : s.second + 60*(s.minute + 60*(s.hour+24)) if s.hour < 12\n",
    "                else s.second + 60*(s.minute + 60*s.hour),\n",
    "                times)\n",
    "        else:\n",
    "            time_list = map(lambda s : s.second + 60*(s.minute + 60*s.hour),\n",
    "                times)\n",
    "\n",
    "        return float(np.std(time_list))\n",
    "    \n",
    "    def from_min_hour_to_num(time):\n",
    "        minutes_per = time[1]/float(60)\n",
    "        \n",
    "        return time[0] + minutes_per\n",
    "    \n",
    "    def calculate_social_jetlag(msw,msf_c):\n",
    "        \"\"\"\n",
    "        Function which returns the social jetlag for a given user. \n",
    "        The social jet lag is defiend as the abs difference between MSW and MSF_C\n",
    "        \"\"\"\n",
    "        \n",
    "        day_w = 1\n",
    "        day_f = 1\n",
    "        if msw[0] < 12:\n",
    "            day_w = 2\n",
    "        if msf_c[0] < 12:\n",
    "            day_f = 2\n",
    "                \n",
    "        dtw = datetime.datetime(2015,10,day_w,msw[0],msw[1],msw[2])\n",
    "        dtf = datetime.datetime(2015,10,day_f,msf_c[0],msf_c[1],msf_c[2])\n",
    "            \n",
    "        #diff = abs((dtf-dtw).total_seconds())\n",
    "        diff = (dtf-dtw).total_seconds()\n",
    "            \n",
    "        return diff\n",
    "    \n",
    "    #Defines the two arrays: Free and work. The weekends is defined as freedays. \n",
    "    arr_free = [x for x in items if x[\"weekday\"] == 4 or x[\"weekday\"] == 5]\n",
    "    arr_work = [x for x in items if x[\"weekday\"] != 4 and x[\"weekday\"] != 5 ]\n",
    "    average_sleep_state = float(np.mean([x[\"average_sleep\"] for x in items]))\n",
    "    \n",
    "    if len(arr_free) > 4 and len(arr_work) > 8:\n",
    "        month_dict = {}\n",
    "        for item in items:\n",
    "            month = data_func_from_string(item[\"start_time\"]).month\n",
    "            if month not in month_dict:\n",
    "                month_dict[month] = []\n",
    "            \n",
    "            month_dict[month].append(item)\n",
    "            \n",
    "        month_chronotype = []\n",
    "        month_social_jetlag = []\n",
    "        \n",
    "        for month in month_dict:\n",
    "            month_items = month_dict[month]\n",
    "\n",
    "            \n",
    "            arr_free_month = [x for x in month_items if x[\"weekday\"] == 4 or x[\"weekday\"] == 5]\n",
    "            arr_work_month = [x for x in month_items if x[\"weekday\"] != 4 and x[\"weekday\"] != 5 ]\n",
    "            \n",
    "            if len(arr_free_month) == 0 or len(arr_work_month) == 0:\n",
    "                continue\n",
    "                \n",
    "            avg_start_work_month = get_average_timestamp([x[\"start_time\"] for x in arr_work_month],True)\n",
    "            avg_end_work_month = get_average_timestamp([x[\"end_time\"] for x in arr_work_month],False)\n",
    "        \n",
    "            avg_start_free_month = get_average_timestamp([x[\"start_time\"] for x in arr_free_month],True)\n",
    "            avg_end_free_month = get_average_timestamp([x[\"end_time\"] for x in  arr_free_month],False)\n",
    "        \n",
    "            msw_month  = get_midpoint(avg_start_work_month,np.mean([x[\"sleep_duration\"] for x in arr_work_month]))\n",
    "            msf_month = get_midpoint(avg_start_free_month,np.mean([x[\"sleep_duration\"] for x in arr_free_month]))\n",
    "\n",
    "            msf_c_month = adjusted_msf(msf_month, np.mean([x[\"sleep_duration\"] for x in arr_free_month]),\n",
    "                                      np.mean([x[\"sleep_duration\"] for x in arr_work_month]))\n",
    "            \n",
    "            month_chronotype.append(from_min_hour_to_num(msf_c_month))\n",
    "            month_social_jetlag.append(calculate_social_jetlag(msw_month,msf_month))\n",
    "        \n",
    "        avg_start_work = get_average_timestamp([x[\"start_time\"] for x in arr_work],True)\n",
    "        avg_end_work = get_average_timestamp([x[\"end_time\"] for x in arr_work],False)\n",
    "        \n",
    "        avg_start_free = get_average_timestamp([x[\"start_time\"] for x in arr_free],True)\n",
    "        avg_end_free = get_average_timestamp([x[\"end_time\"] for x in  arr_free],False)\n",
    "        \n",
    "        msw  = get_midpoint(avg_start_work,np.mean([x[\"sleep_duration\"] for x in arr_work]))\n",
    "        msf = get_midpoint(avg_start_free,np.mean([x[\"sleep_duration\"] for x in arr_free]))\n",
    "        \n",
    "        msf_c = adjusted_msf(msf, np.mean([x[\"sleep_duration\"] for x in arr_free]),\n",
    "                                  np.mean([x[\"sleep_duration\"] for x in arr_work]))\n",
    "\n",
    "        sj_month = None\n",
    "        c_month = None\n",
    "        \n",
    "        if len(month_chronotype) > 0:\n",
    "            sj_month = float(np.mean(month_social_jetlag))\n",
    "            c_month = float(np.mean(month_chronotype))\n",
    "            \n",
    "        return list(items)[0][\"useruuid\"]\\\n",
    "            ,average_sleep_state\\\n",
    "            ,from_min_hour_to_num(avg_start_work)\\\n",
    "            ,from_min_hour_to_num(avg_start_free)\\\n",
    "            ,from_min_hour_to_num(avg_end_work)\\\n",
    "            ,from_min_hour_to_num(avg_end_free)\\\n",
    "            ,from_min_hour_to_num(msw)\\\n",
    "            ,from_min_hour_to_num(msf)\\\n",
    "            ,from_min_hour_to_num(msf_c)\\\n",
    "            ,c_month\\\n",
    "            ,sj_month\\\n",
    "            ,calculate_social_jetlag(msw,msf)\\\n",
    "            ,get_std_timestamp([x[\"start_time\"] for x in arr_work],True)\\\n",
    "            ,get_std_timestamp([x[\"end_time\"] for x in arr_work],False)\\\n",
    "            ,get_std_timestamp([x[\"start_time\"] for x in arr_free],True)\\\n",
    "            ,get_std_timestamp([x[\"end_time\"] for x in  arr_free],False)\\\n",
    "            ,get_std_timestamp([x[\"start_time\"] for x in items],True)\\\n",
    "            ,get_std_timestamp([x[\"end_time\"] for x in items],False)\n",
    "    else:\n",
    "        return list(items)[0][\"useruuid\"],average_sleep_state, None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following takes the locations and divides them into timebins of size 5 mins. If no locations for a given user, the nearest locations will be found. If they are more than 3 hours old, the given timebin will be discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds the two files sun and place so we can get the amount of daylight to each of the coordinate set. \n",
    "sc.addPyFile(\"sun.py\")\n",
    "sc.addPyFile(\"place.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_schema = StructType([StructField('time',TimestampType() , True),\n",
    "                     StructField('time_key', StringType(), True),\n",
    "                     StructField('useruuid', StringType(), True),\n",
    "                     StructField('timebin', StringType(), True),\n",
    "                     StructField('lat', FloatType(), True),\n",
    "                     StructField('lon', FloatType(), True),\n",
    "                     StructField('name', StringType(), True),\n",
    "                     StructField('area', StringType(), True),\n",
    "                     StructField('country', StringType(), True),\n",
    "                     StructField('region', StringType(), True),\n",
    "                     StructField('sunlight', FloatType(), True),\n",
    "                     StructField('from_rise', FloatType(), True),\n",
    "                     StructField('from_set', FloatType(), True),\n",
    "                     StructField('altitude', FloatType(), True),         \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, folder in enumerate(folders_to_load[:1]):\n",
    "    print \"########### Starting with \", folder, \"###########\"\n",
    "\n",
    "    #The period for the timebins which is created. \n",
    "    times_for_period = start_end_times_for_periods[i]\n",
    "    \n",
    "    #Load the dataset for the given period. \n",
    "    df_location_raw = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(pre_path+\"/location/\"+folder+\"/*.avro\")).where(\"accuracy > 0\").drop(\"devices\")\n",
    "    \n",
    "    #Load sleep for the given period so we know which users we want locations from. \n",
    "    df_sleep_period = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_\"+folder+\"/*.avro\")).select(F.col(\"useruuid\").alias(\"dummy_id\")).distinct()\n",
    "    \n",
    "    #Join the two dataframes so we only work with the relevant locations\n",
    "    df_location_raw_filtered = df_location_raw.join(df_sleep_period, \n",
    "                (df_location_raw[\"useruuid\"] == df_sleep_period[\"dummy_id\"]))\\\n",
    "        .drop(\"dummy_id\")\n",
    "        \n",
    "    #Defines the timebin size in minutes. As a start it is 5 minutes. \n",
    "    timebin_interval = 5\n",
    "    \n",
    "    #Generates all the bins for the month\n",
    "    start = start_end_date_times_for_periods[i][0]\n",
    "    last = start_end_date_times_for_periods[i][1]\n",
    "    delta = datetime.timedelta(minutes=timebin_interval) \n",
    "\n",
    "    times = []\n",
    "    while start < last:\n",
    "        times.append(start)\n",
    "        start += delta\n",
    "        \n",
    "    times = sc.broadcast(times)\n",
    "    \n",
    "    \n",
    "    rdd_location = df_location_raw_filtered.rdd\\\n",
    "        .map(lambda item : (item[\"useruuid\"],item))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(location_mapper)\\\n",
    "        .flatMap(lambda x : x[1])\n",
    "\n",
    "\n",
    "    mapped_df = rdd_location.toDF(location_schema)\n",
    "    print \"Mapped data\"\n",
    "    mapped_df.write.format('com.databricks.spark.avro').save(bucket_path+\"location_\"+folder)\n",
    "    print \"Done with \", folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the sleep and location datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for folder in folders_to_load:\n",
    "    df_period = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_\"+folder+\"/*.avro\"))\\\n",
    "                .withColumn('sleep_bin', time_bin_udf(F.col('start_time'), F.col(\"useruuid\")))\n",
    "    \n",
    "    df_period_locations = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"location_\"+folder+\"/*.avro\")).drop(\"time\").drop(\"time_key\").drop(\"useruuid\")\n",
    "\n",
    "    \n",
    "    df_period_sleep_loc = df_period.join(df_period_locations, \n",
    "                (df_period[\"sleep_bin\"] == df_period_locations[\"timebin\"]))\\\n",
    "        .drop(\"timebin\").drop(\"sleep_bin\")\n",
    "        \n",
    "    print \"Done joining \", folder\n",
    "    df_period_sleep_loc.write.format('com.databricks.spark.avro').save(bucket_path+\"sleep_loc_\"+folder)\n",
    "    print \"Done with \", folder    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get locations with no country registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After february 2016 the location dataset did not contain country information. So this is needed to be added manually. To do this i am using a shape file. Se Location notebook for convertion between coordinates and country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_folders = []\n",
    "for folder in folders_to_load:\n",
    "    arr_folders.append(bucket_path+\"sleep_loc_\"+folder+\"/*.avro\")\n",
    "    \n",
    "df_loc_total = (sqlContext.read.format(\"com.databricks.spark.avro\").load(arr_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare coordinates to file\n",
    "coordinates_no_country = df_loc_total.select(\"lon\",\"lat\").where(\"country == ''\").collect()\n",
    "coordinates_all = df_loc_total.select(\"lon\",\"lat\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for coordinates without country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coordinates = []\n",
    "for row in coordinates_no_country:\n",
    "    coordinates.append(str(row[\"lon\"])+\",\"+str(row[\"lat\"]))\n",
    "    \n",
    "print len(coordinates)\n",
    "\n",
    "f = open('coordinates__no_country.txt', 'w')\n",
    "for item in coordinates:\n",
    "    f.write(\"%s\\n\" % item)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next for all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coordinates = []\n",
    "for row in coordinates_all:\n",
    "    coordinates.append(str(row[\"lon\"])+\",\"+str(row[\"lat\"]))\n",
    "    \n",
    "print len(coordinates)\n",
    "\n",
    "f = open('coordinates_all.txt', 'w')\n",
    "for item in coordinates:\n",
    "    f.write(\"%s\\n\" % item)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the countries for each of the location set from dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coordinates_with_country = pickle.load(open(\"coordinates_no_country_filled.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defines a dict with country as key and region as value\n",
    "countries_region = df_loc_total.select(\"country\",\"region\").distinct().collect()\n",
    "country_region_dict = {}\n",
    "for row in countries_region:\n",
    "    country_region_dict[row[\"country\"]] = row[\"region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make broadcast values\n",
    "coordinates_with_country_bc = sc.broadcast(coordinates_with_country)\n",
    "country_region_dict_bc = sc.broadcast(country_region_dict)\n",
    "replace_country_dict_bc = sc.broadcast(countries_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#maps the items and save the df\n",
    "for folder in folders_to_load: \n",
    "    df_period_locations = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_loc_\"+folder+\"/*.avro\"))\n",
    "    \n",
    "    df_period_locations_updated = df_period_locations.rdd.map(no_country_mapper).toDF(df_period_locations.schema)\n",
    "    print \"Saving\"\n",
    "    df_period_locations_updated.write.format('com.databricks.spark.avro').save(bucket_path+\"sleep_loc_filled_\"+folder)\n",
    "    print \"Done with folder \", folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_country_mapper(item):\n",
    "    if item[\"country\"] == \"\": #If no registered country. \n",
    "        item = item.asDict()\n",
    "        key = str(item[\"lon\"]) + \"-\" + str(item[\"lat\"]) + \"\\n\" \n",
    "        if key in coordinates_with_country_bc.value:\n",
    "            #We have a country for the location. \n",
    "            country = coordinates_with_country_bc.value[key]\n",
    "            \n",
    "            #If the country name should be converted..\n",
    "            if country in replace_country_dict_bc.value:\n",
    "                country = replace_country_dict_bc.value[country]\n",
    "                \n",
    "            item[\"country\"] = country\n",
    "            if country in country_region_dict_bc.value:\n",
    "                item[\"region\"] = country_region_dict_bc.value[item[\"country\"]]\n",
    "        return item\n",
    "    else:\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the location mapper which creates a row for each timebin for each of the user. For each row, the median of the locations beloing to that bin is selected as the coordinates. If there is not coordinates for a given bin, it fnds the closests locations. If the clostest location is more than 3 hours away, the row is disgarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_mapper(items):\n",
    "    \n",
    "    from sun import Sun\n",
    "    import place as place\n",
    "\n",
    "    #Generates an array for each of the timebins. Are going to be filled with locations beloning to that timebin. \n",
    "    has_locations = set()\n",
    "    temp_data = {}\n",
    "    for time in times.value:\n",
    "        time_key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "        temp_data[time_key] = []\n",
    "    \n",
    "    def utc_date_func_from_string(d):\n",
    "        \"\"\"Converts a date string to a unaware utc datetime. Used for the timebin which is using the utc time.\"\"\"\n",
    "        from dateutil.parser import parse\n",
    "        date = parse(d)\n",
    "        date_utc = date - date.utcoffset()\n",
    "        unaware_date = date_utc.replace(tzinfo=None)\n",
    "    \n",
    "        return unaware_date\n",
    "    \n",
    "    #Loop through each of the locations beloning to the given user. \n",
    "    for loc in items:\n",
    "        start_t = utc_date_func_from_string(loc[\"start_time\"])\n",
    "        end_t = utc_date_func_from_string(loc[\"end_time\"])     \n",
    "        \n",
    "        #Find the timebin which the start time belongs to and adds the location to the given timebin. \n",
    "        minute = int(5 * round(float(start_t.minute)/5))\n",
    "        diff = minute - start_t.minute\n",
    "        time = start_t + datetime.timedelta(minutes=diff)\n",
    "        \n",
    "        delta = datetime.timedelta(minutes=timebin_interval) \n",
    "        d_key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "        if d_key in temp_data:\n",
    "            has_locations.add(d_key)\n",
    "            temp_data[d_key].append(loc)\n",
    "            time += delta\n",
    "        \n",
    "        #Loop through each of the timebins between start and enddate and adds them to the given timebin. \n",
    "        while time < end_t:\n",
    "            d_key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "            if d_key in temp_data:\n",
    "                has_locations.add(d_key)\n",
    "                temp_data[d_key].append(loc)\n",
    "            time += delta\n",
    "    \n",
    "    if len(has_locations) == 0:\n",
    "        return []\n",
    "    \n",
    "    #Variables which holds the last found location. Added so we do not need to look up the closests for each run. \n",
    "    last_coordinates = None\n",
    "    last_time = None\n",
    "    \n",
    "    rows = []\n",
    "    has_locations = [datetime.datetime.strptime(x,'%Y-%m-%d-%H-%M') for x in has_locations]\n",
    "    \n",
    "    for time in times.value:\n",
    "        time_key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "        coordinates = temp_data[time_key]\n",
    "        row = None\n",
    "        \n",
    "        #If length of coordinates is equal to 0, we need to find the closests registered location. \n",
    "        if len(coordinates) == 0:\n",
    "            if last_coordinates == None:\n",
    "                tt = min(has_locations, key=lambda x: abs(x - time))\n",
    "                d_key = str(tt.year) + \"-\" + str(tt.month) + \"-\" + str(tt.day) + \"-\" + str(tt.hour) + \"-\" + str(tt.minute)\n",
    "                last_time = tt\n",
    "                last_coordinates = temp_data[d_key]\n",
    "                diff_last = abs((last_time-time).total_seconds())\n",
    "                if diff < 36000:\n",
    "                    coordinates = temp_data[d_key]\n",
    "                \n",
    "            else:\n",
    "                diff_last = abs((last_time-time).total_seconds())\n",
    "                if diff < 36000: \n",
    "                    #Last found coordinates are okay, we are going to use them. \n",
    "                    coordinates = last_coordinates \n",
    "                else: \n",
    "                    #The last found location are to far away. Check to see whether there are any closer. \n",
    "                    tt = min(has_locations, key=lambda x: abs(x - time))\n",
    "                    d_key = str(tt.year) + \"-\" + str(tt.month) + \"-\" + str(tt.day) + \"-\" + str(tt.hour) + \"-\" + str(tt.minute)\n",
    "                    last_time = tt\n",
    "                    last_coordinates = temp_data[d_key]\n",
    "                        \n",
    "                    diff_last = abs((last_time-time).total_seconds())\n",
    "                    if diff < 36000:\n",
    "                        coordinates = temp_data[d_key]                   \n",
    "            \n",
    "        if len(coordinates) > 0:\n",
    "            time_key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "            last_coordinates = coordinates\n",
    "            last_time = time\n",
    "            lat = np.median([x[\"latitude\"] for x in coordinates])\n",
    "            lon = np.median([x[\"longitude\"] for x in coordinates])\n",
    "            \n",
    "            #Get the risetime, settime and sunlight \n",
    "            pl = place.Place(\"\",(lon,lat))\n",
    "            risetime, settime, hrs, mins  = place.getsuninfo(pl, time)\n",
    "\n",
    "            return_hrs = float(hrs)\n",
    "            if mins < 8:\n",
    "                return_hrs += 0.0\n",
    "            elif mins < 16:\n",
    "                 return_hrs += 0.25\n",
    "            elif mins < 31:\n",
    "                return_hrs += 0.50\n",
    "            elif mins < 46:\n",
    "                return_hrs += 0.75\n",
    "            else:\n",
    "                return_hrs += 1.0\n",
    "                \n",
    "            altitude = np.mean([x[\"altitude\"] for x in coordinates])\n",
    "            \n",
    "            row = [\n",
    "                time,\n",
    "                time_key,\n",
    "                coordinates[0][\"useruuid\"],\n",
    "                time_key + \"_\" + coordinates[0][\"useruuid\"],\n",
    "                float(lat),\n",
    "                float(lon),\n",
    "                coordinates[0][\"name\"],\n",
    "                coordinates[0][\"area\"],\n",
    "                coordinates[0][\"country\"],\n",
    "                coordinates[0][\"region\"],\n",
    "                float(return_hrs),\n",
    "                (time-risetime).total_seconds(),\n",
    "                (time-settime).total_seconds(),\n",
    "                float(altitude)\n",
    "            ]\n",
    "\n",
    "        if row != None:\n",
    "            rows.append(row)\n",
    "            \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# App usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates a row which is a timebin with length 5 minutes for each of the periods for each of the users. Each row contains information about how much the phone has been used for the last hour. The row has a key which can link the app usage for the last hour with an sleep entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, folder in enumerate(folders_to_load):\n",
    "    print \"########### Starting with \", folder, \"###########\"\n",
    "\n",
    "    #The period for the timebins which is created. \n",
    "    times_for_period = start_end_times_for_periods[i]\n",
    "    \n",
    "    #Load the dataset for the given period. \n",
    "    df_app_raw = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(pre_path+\"/applicationactivity/\"+folder+\"/*.avro\")).where(\"category != 'N/A'\")\\\n",
    "        .where(\"deleted_time == ''\")\n",
    "    \n",
    "    #Load sleep for the given period so we know which users we want app activity from. \n",
    "    df_user_ids = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_\"+folder+\"/*.avro\"))\\\n",
    "                .select(F.col(\"useruuid\").alias(\"dummy_id\")).distinct()\n",
    "                #.withColumn('sleep_bin', time_bin_minute_udf(F.col('start_time'), F.col(\"useruuid\")))\n",
    "    \n",
    "    print \"Done loading\"\n",
    "    \n",
    "    #Join the two dataframes so we only work with the relevant app usages\n",
    "    df_app_raw_filtered = df_app_raw.join(df_user_ids, \n",
    "                (df_app_raw[\"useruuid\"] == df_user_ids[\"dummy_id\"]))\\\n",
    "        .drop(\"dummy_id\")\n",
    "               \n",
    "    #Make a dataframe with the newest enty for each of the id's\n",
    "    newest_ids = df_app_raw_filtered.select(F.col(\"id\").alias(\"dummy_id\"),\"timestamp_seen\")\\\n",
    "        .groupBy(F.col(\"dummy_id\"))\\\n",
    "        .agg(F.max(F.col(\"timestamp_seen\")))\n",
    "        \n",
    "    print \"Newest entries found\"\n",
    "    \n",
    "    #join the two dataframes\n",
    "    df_app_no_doublets = df_app_raw_filtered.join(newest_ids, \n",
    "                                (df_app_raw_filtered[\"timestamp_seen\"] == newest_ids[\"max(timestamp_seen)\"]) & \n",
    "                                (df_app_raw_filtered[\"id\"] == newest_ids[\"dummy_id\"]))\\\n",
    "                                .drop(\"max(timestamp_seen)\").drop(\"dummy_id\")\n",
    "        \n",
    "    print \"Removed duplicates, Saving\"\n",
    "    df_app_no_doublets.write.format('com.databricks.spark.avro').save(bucket_path+\"app_\"+folder)\n",
    "    \n",
    "    print \"Done with \", folder\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_apps_folders = []\n",
    "for folder in folders_to_load:\n",
    "    all_apps_folders.append(bucket_path+\"app_\"+folder+\"/*.avro\")\n",
    "    \n",
    "app_total_df = (sqlContext.read.format(\"com.databricks.spark.avro\").load(all_apps_folders))\n",
    "print app_total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_apps = app_total_df.select(\"application_name\").distinct().collect()\n",
    "print len(unique_apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique_apps = app_total_df.select(\"application_name\").distinct().collect()\n",
    "\n",
    "f = open('apps_all.txt', 'w')\n",
    "for app in unique_apps:\n",
    "    f.write(\"%s\\n\" % app[\"application_name\"])\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of exposure for a time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load in the cleaned app dataframes for each of the periodes and find the amount of exposure for the following time periods: 1 hour, 2 hours and 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "#structure of the generated df\n",
    "periods_prefix = [\"one\",\"two\",\"three\"]\n",
    "\n",
    "arr_app_schema = [\n",
    "    StructField('timebin', StringType(), True)\n",
    "]\n",
    "\n",
    "for pre in periods_prefix:\n",
    "    arr_app_schema.append(StructField(\"seconds_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"seconds_per_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"browsing_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"entertainment_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"other_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"books_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"music_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"game_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"camera_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"communication_\"+pre,FloatType(),True))\n",
    "    arr_app_schema.append(StructField(\"movie_\"+pre,FloatType(),True))\n",
    "    \n",
    "app_schema = StructType(arr_app_schema)\n",
    "print len(app_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, folder in enumerate(folders_to_load):\n",
    "    print \"########### Starting with \", folder, \"###########\"\n",
    "    \n",
    "    #Load the dataset for the given period and only select the attributes we need to decrease the size in memory!!!\n",
    "    df_app_no_doublets = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(bucket_path+\"app_\"+folder)).select(\"start_time\",\"end_time\",\"useruuid\",\"category\") \n",
    "    \n",
    "    print \"Done loading\"\n",
    "    \n",
    "    #Defines the timebin size in minutes.\n",
    "    timebin_interval = 5\n",
    "    \n",
    "    #Generates all the bins for the period.\n",
    "    start = start_end_date_times_for_periods[i][0]\n",
    "    last = start_end_date_times_for_periods[i][1]\n",
    "    delta = datetime.timedelta(minutes=timebin_interval) \n",
    "\n",
    "    times = []\n",
    "    while start < last:\n",
    "        times.append(start)\n",
    "        start += delta\n",
    "        \n",
    "    times = sc.broadcast(times)\n",
    "    \n",
    "    rdd_app = df_app_no_doublets.rdd\\\n",
    "        .map(lambda item : (item[\"useruuid\"],item))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(application_mapper)\\\n",
    "        .flatMap(lambda x : x[1])\n",
    "        \n",
    "    print \"Convertig to df\"\n",
    "    mapped_df = rdd_app.toDF(app_schema).drop(\"useruuid\").drop(\"time_key\")\n",
    "    print \"Saving\"\n",
    "    mapped_df.write.format('com.databricks.spark.avro').save(bucket_path+\"/app_usage/app_bins_two_three_\"+folder)\n",
    "        \n",
    "    print \"Done with \", folder\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the sleep and app dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, folder in enumerate(folders_to_load):\n",
    "    print \"########### Starting with \", folder, \"###########\"\n",
    "    \n",
    "    #App bins for the given period\n",
    "    df_app = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(bucket_path+\"/app_usage/app_bins_one_\"+folder))\n",
    "    \n",
    "    #Load sleep for the given period so we can merge them togheter. \n",
    "    df_sleep_period = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_\"+folder+\"/*.avro\"))\\\n",
    "                .withColumn('sleep_bin', time_bin_udf(F.col('start_time'), F.col(\"useruuid\")))\n",
    "\n",
    "    print \"Joining sleep and app\"\n",
    "    df_app_sleep = df_sleep_period.join(df_app, \n",
    "            (df_sleep_period[\"sleep_bin\"] == df_app[\"timebin\"]))\\\n",
    "            .drop(\"sleep_bin\").drop(\"timebin\")\n",
    "    \n",
    "    print \"Datasets joined, saving\"\n",
    "    df_app_sleep.write.format('com.databricks.spark.avro').save(bucket_path+\"/app_usage/app_sleep_one_\"+folder)\n",
    "    \n",
    "    print \"Done with \", folder\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many sleep entries which has been removed due to joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9696\n"
     ]
    }
   ],
   "source": [
    "df_app_sleep = df_sleep_period.join(df_app, \n",
    "            (df_sleep_period[\"sleep_bin\"] == df_app[\"timebin\"]),\"left_outer\")\\\n",
    "            .drop(\"sleep_bin\").drop(\"timebin\")\n",
    "        \n",
    "users_dropped = df_app_sleep.where(F.col(\"seconds_one\").isNull()).select(\"useruuid\").distinct().collect()\n",
    "print len(users_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the application mapper which maps and application usage for the last hour to timebins of size 1 minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_mapper(items): \n",
    "    temp_data = {}\n",
    "    bin_size = 5\n",
    "    \n",
    "    #Generates the timebins. The timebins has an interval of bin size minute. \n",
    "    for time in times.value:\n",
    "        time_key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "        temp_data[time_key] = {\n",
    "            \"seconds\" : 0, \"Browsing\" : 0, \"Entertainment\" : 0, \"Other\" : 0, \"Books\" : 0, \"Music\" : 0, \"Game\" : 0,\n",
    "            \"Camera/Album\" : 0, \"Communication\" : 0, \"Movie/TV\" : 0       \n",
    "        }\n",
    "    \n",
    "    def utc_date_func_from_string(d):\n",
    "        \"\"\"Converts a date string to a unaware utc datetime. Used for the timebin which is using the utc time.\"\"\"\n",
    "        from dateutil.parser import parse\n",
    "        date = parse(d)\n",
    "        date_utc = date - date.utcoffset()\n",
    "        unaware_date = date_utc.replace(tzinfo=None)\n",
    "    \n",
    "        return unaware_date\n",
    "    \n",
    "    for application in items:\n",
    "        #Generates the start and end time for the app usage\n",
    "        start_t = utc_date_func_from_string(application[\"start_time\"])\n",
    "        end_t = utc_date_func_from_string(application[\"end_time\"]) \n",
    "        \n",
    "        #Is the delta we increment the time with. \n",
    "        delta = datetime.timedelta(seconds=1)\n",
    "        \n",
    "        #Fill the corresponding timebins. \n",
    "        while start_t <= end_t:  \n",
    "            \n",
    "            #Round to the corresponding timebin (bin_size minute sizes)\n",
    "            minute = int(bin_size * round(float(start_t.minute)/bin_size))\n",
    "            diff = minute - start_t.minute\n",
    "            time = start_t + datetime.timedelta(minutes=diff)\n",
    "        \n",
    "            #Generates           \n",
    "            time_key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "            \n",
    "            if time_key in temp_data:\n",
    "                temp_data[time_key][\"seconds\"] += 1\n",
    "                temp_data[time_key][application[\"category\"]] += 1\n",
    "            \n",
    "            start_t += delta\n",
    "            \n",
    "    rows = []\n",
    "    useruuid = list(items)[0][\"useruuid\"]\n",
    "    periods_num = [61,121,181]\n",
    "    periods_prefix = [\"one\",\"two\",\"three\"]\n",
    "    \n",
    "    \n",
    "    #Defines the keys for each of the categories. \n",
    "    keys = [\"Browsing\", \"Entertainment\",\"Other\",\"Books\",\"Music\",\"Game\",\"Camera/Album\",\"Communication\",\"Movie/TV\"]\n",
    "    \n",
    "    #Loop through each timebin\n",
    "    for key in temp_data:\n",
    "        incomplete = False\n",
    "        #Get data for current timbin\n",
    "        item = temp_data[key]\n",
    "        total_dict = {}\n",
    "        #Defines the dict which holds the information from the current time and 1,2 and 3 hours back\n",
    "        for j,pre in enumerate(periods_prefix):\n",
    "            total_dict[\"seconds_\"+pre] = item[\"seconds\"]\n",
    "            for k in keys:\n",
    "                total_dict[k+\"_\"+pre] = item[k]\n",
    "\n",
    "        time = datetime.datetime.strptime(key,'%Y-%m-%d-%H-%M')\n",
    "        for j, num in enumerate(periods_num):\n",
    "            pre = periods_prefix[j]\n",
    "            for i in range(1,num/bin_size+1): \n",
    "                #Substract i*bin_size minutes from the current key\n",
    "                formatted_time = time - datetime.timedelta(minutes=bin_size*i)\n",
    "                time_key = str(formatted_time.year) + \"-\" + str(formatted_time.month) + \"-\" + str(formatted_time.day) + \"-\" + str(formatted_time.hour) + \"-\" + str(formatted_time.minute)\n",
    "                if time_key not in temp_data:\n",
    "                    incomplete = True\n",
    "                    continue\n",
    "                    \n",
    "                new_item = temp_data[time_key]\n",
    "            \n",
    "                total_dict[\"seconds_\"+pre] += new_item[\"seconds\"]\n",
    "                for k in keys:\n",
    "                    total_dict[k+\"_\"+pre] += new_item[k]\n",
    "     \n",
    "        row = [\n",
    "            key  + \"_\" + useruuid\n",
    "        ]\n",
    "        \n",
    "        for j,pre in enumerate(periods_prefix):\n",
    "            total = total_dict[\"seconds_\"+pre]\n",
    "            row.append(float(total))\n",
    "            if total > 0:\n",
    "                row.append(float(total) / float(60*(periods_num[j]-1)))\n",
    "            else:\n",
    "                row.append(float(0))\n",
    "            \n",
    "            #Define the total for each of the categories. \n",
    "            for k in keys:\n",
    "                if total_dict[k+\"_\"+pre] > 0:\n",
    "                    row.append(float(total_dict[k+\"_\"+pre]))\n",
    "                else:\n",
    "                    row.append(float(0))\n",
    "        if incomplete == False:\n",
    "            rows.append(row)\n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last time using a mobile device before sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find the intervall from the users looked at their phone last to they god to bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apps_alarms = set()\n",
    "with open(\"apps_alarm.txt\",\"rb\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        app = line.split(\"\\n\")[0]\n",
    "        if app != \"\":\n",
    "            apps_alarms.add(app)\n",
    "apps_alarms_bc = sc.broadcast(apps_alarms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema_app = StructType([StructField('sleep_id', StringType(), True),\n",
    "                    # StructField('alarm_used', BooleanType(), True),\n",
    "                     StructField('last_app_time_since', FloatType(), True),\n",
    "                     StructField('last_app_date', StringType(), True),\n",
    "                     StructField('last_app_category', StringType(), True),\n",
    "                     StructField('last_app_name', StringType(), True),\n",
    "                     #StructField('first_app_time_since', FloatType(), True),\n",
    "                    # StructField('first_app_date', StringType(), True),\n",
    "                    # StructField('first_app_category', StringType(), True),\n",
    "                    # StructField('first_app_name', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, folder in enumerate(folders_to_load):\n",
    "    print \"########### Starting with \", folder, \"###########\"\n",
    "   \n",
    "    df_app_usage_by_user = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(bucket_path+\"app_\"+folder)).select(\"end_time\",F.col(\"useruuid\").alias(\"uid\"),\"application_name\",\"category\")\\\n",
    "    .select(\"*\", F.format_string(\"%sg#g%sg#g%s\",F.col(\"end_time\"),F.col(\"application_name\"),F.col(\"category\")).alias(\"app_key\"))\\\n",
    "      .groupby(\"uid\").agg(F.collect_list(\"app_key\")).drop(\"end_time\")\n",
    "\n",
    "    print \"Apps loaded, loading sleep\"\n",
    "\n",
    "    #Load sleep for the given period and add the new columns\n",
    "    df_sleep_period = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_\"+folder+\"/*.avro\")).select(\"start_time\",\"id\",\"useruuid\")\n",
    "    \n",
    "    print \"Sleep loaded, joining\"\n",
    "    df_sleep_period = df_sleep_period.join(df_app_usage_by_user, \n",
    "                                    (df_sleep_period[\"useruuid\"] == df_app_usage_by_user[\"uid\"]),\"left_outer\")\\\n",
    "                                    .drop(\"uid\")\n",
    "        \n",
    "    print \"Joined\"\n",
    "    \n",
    "    df_sleep_period = df_sleep_period.rdd.map(app_usage_mapper).toDF(schema_app)\n",
    "        \n",
    "    print \"Column added, saving\"\n",
    "    \n",
    "    df_sleep_period.write.format('com.databricks.spark.avro').save(bucket_path+\"/app_usage/app_usage_before_bed_3\"+folder)\n",
    "    print \"Saved...\"\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def app_usage_mapper(item):\n",
    "    \n",
    "    target_date = parse(item[\"start_time\"]).replace(tzinfo=None)\n",
    "\n",
    "    last_app_date = None\n",
    "    last_app_time_since = None\n",
    "    last_app_name = None\n",
    "    last_app_category = None\n",
    "   \n",
    "    apps_for_user = item[\"collect_list(app_key)\"]\n",
    "    \n",
    "    if apps_for_user == None:\n",
    "        return item[\"id\"],None,None,None,None \n",
    "    \n",
    "    for row in apps_for_user:\n",
    "        row_date_str, row_name, row_category = row.split(\"g#g\")\n",
    "        \n",
    "        row_date = parse(row_date_str).replace(tzinfo=None)\n",
    "          \n",
    "        if row_date < target_date:        \n",
    "            delta = (target_date-row_date).total_seconds()\n",
    "            if last_app_time_since == None:\n",
    "                last_app_time_since = float(delta)\n",
    "                last_app_date = row_date_str\n",
    "                last_app_category = row_category\n",
    "                last_app_name = row_name\n",
    "                \n",
    "            elif delta < last_app_time_since:\n",
    "                last_app_time_since = float(delta)\n",
    "                last_app_date = row_date_str\n",
    "                last_app_category = row_category\n",
    "                last_app_name = row_name\n",
    "            \n",
    "    return [item[\"id\"],last_app_time_since, last_app_date, last_app_category,last_app_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def app_usage_mapper(item):\n",
    "    \n",
    "    target_date = parse(item[\"start_time\"]).replace(tzinfo=None)\n",
    "    target_date_end = parse(item[\"end_time\"]).replace(tzinfo=None)\n",
    "    \n",
    "    #Defines intervals for the wake up period. \n",
    "    target_end_interval_lower = target_date_end - datetime.timedelta(minutes=15)\n",
    "    target_end_interval_upper = target_date_end + datetime.timedelta(minutes=15)\n",
    "\n",
    "    last_app_date = None\n",
    "    last_app_time_since = None\n",
    "    last_app_name = None\n",
    "    last_app_category = None\n",
    "    \n",
    "    first_app_date = None\n",
    "    first_app_time_since = None\n",
    "    first_app_name = None\n",
    "    first_app_category = None\n",
    "    \n",
    "    alarm_used = False\n",
    "    alarm_app = None\n",
    "    \n",
    "    apps_for_user = item[\"collect_list(app_key)\"]\n",
    "    \n",
    "    if apps_for_user == None:\n",
    "        return item[\"id\"], None,None,None,None,None,None,None,None,None,None \n",
    "    \n",
    "    for row in apps_for_user:\n",
    "        row_date_str, row_name, row_category = row.split(\"#\")\n",
    "        \n",
    "        row_date = parse(row_date_str).replace(tzinfo=None)\n",
    "        \n",
    "        if row_date > target_end_interval_lower and row_date < target_end_interval_upper:\n",
    "            #App usage in the wakeup interval. Check to see whether it is an alarm app. \n",
    "            if row_name in apps_alarms_bc.value:\n",
    "                alarm_used = True\n",
    "                alarm_app = row_name\n",
    "\n",
    "        if row_date > target_date_end:\n",
    "            delta = (target_date_end-row_date).total_seconds()\n",
    "            if first_app_time_since == None:\n",
    "                first_app_time_since = float(delta)\n",
    "                first_app_date = row_date_str\n",
    "                first_app_category = row_category\n",
    "                first_app_name = row_name\n",
    "            elif delta < first_app_time_since:\n",
    "                first_app_time_since = float(delta)\n",
    "                first_app_date = row_date_str            \n",
    "                first_app_category = row_category\n",
    "                first_app_name = row_name         \n",
    "        elif row_date < target_date:        \n",
    "            delta = (target_date-row_date).total_seconds()\n",
    "            if last_app_time_since == None:\n",
    "                last_app_time_since = float(delta)\n",
    "                last_app_date = row_date_str\n",
    "                last_app_category = row_category\n",
    "                last_app_name = row_name\n",
    "                \n",
    "            elif delta < last_app_time_since:\n",
    "                last_app_time_since = float(delta)\n",
    "                last_app_date = row_date_str\n",
    "                last_app_category = row_category\n",
    "                last_app_name = row_name\n",
    "            \n",
    "    return [item[\"id\"],alarm_used,alarm_app,last_app_time_since, last_app_date, last_app_category,last_app_name,\n",
    "            first_app_time_since,first_app_date,first_app_category,first_app_name] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_all_files_sleep_arr = []\n",
    "for folder in folders_to_load: \n",
    "    load_all_files_sleep_arr.append(bucket_path+\"sleep_\"+folder+\"/*.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_folders = []\n",
    "for folder in folders_to_load:\n",
    "    user_folders.append(pre_path+\"/user/\"+folder+\"/*.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_total = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(load_all_files_sleep_arr)) \n",
    "\n",
    "user_ids_to_load = df_total.select(F.col(\"useruuid\").alias(\"dummy_id\")).distinct()\n",
    "print user_ids_to_load.count()\n",
    "\n",
    "#Load users from the given period\n",
    "users_df_raw = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(user_folders))\n",
    "\n",
    "#Remove users with no sleep\n",
    "df_users_raw_filtered = users_df_raw.join(user_ids_to_load, \n",
    "                (users_df_raw[\"useruuid\"] == user_ids_to_load[\"dummy_id\"]))\\\n",
    "        .drop(\"dummy_id\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#Make a dataframe with the newest enty for each of the users's\n",
    "newest_ids = df_users_raw_filtered.select(F.col(\"useruuid\").alias(\"dummy_id\"),\"timestamp_modified\")\\\n",
    "        .groupBy(F.col(\"dummy_id\"))\\\n",
    "        .agg(F.max(F.col(\"timestamp_modified\")))\n",
    "        \n",
    "    \n",
    "#join the two dataframes\n",
    "df_users_no_doublets = df_users_raw_filtered.join(newest_ids, \n",
    "                                    (df_users_raw_filtered[\"timestamp_modified\"] == newest_ids[\"max(timestamp_modified)\"]) & \n",
    "                                    (df_users_raw_filtered[\"useruuid\"] == newest_ids[\"dummy_id\"]))\\\n",
    "                                    .drop(\"dummy_id\").drop(\"max(timestamp_modified)\")\n",
    "    \n",
    "\n",
    "df_user_final = df_users_no_doublets.withColumn('age', udf_age_of_user(F.col('birthday')))\n",
    "\n",
    "print df_user_final.count()\n",
    "\n",
    "df_user_final.write.format('com.databricks.spark.avro').save(bucket_path+\"users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a users nationality based on the most common country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_folders = []\n",
    "for folder in folders_to_load:\n",
    "    arr_folders.append(bucket_path+\"sleep_loc_filled_\"+folder+\"/*.avro\")\n",
    "    \n",
    "df_loc_total = (sqlContext.read.format(\"com.databricks.spark.avro\").load(arr_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('user', StringType(), True),StructField('country', StringType(), True)])\n",
    "        \n",
    "        \n",
    "def users_nationality_mapper(items):\n",
    "    from collections import Counter\n",
    "    \n",
    "    locations = []\n",
    "    for item in items:\n",
    "        if item[\"country\"] != \"\":\n",
    "            locations.append(item[\"country\"])\n",
    "    \n",
    "    if len(locations) == 0:\n",
    "        return None\n",
    "    \n",
    "    counts = Counter(locations)\n",
    "    return counts.most_common()[:1][0][0]\n",
    "\n",
    "df_user_nationality = df_loc_total.select(\"useruuid\",\"country\").rdd\\\n",
    "        .map(lambda item: (item[\"useruuid\"],item))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(users_nationality_mapper).toDF(schema)\n",
    "        \n",
    "df_user_nationality.write.format('com.databricks.spark.avro').save(bucket_path+\"user_nationality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpple check to see which informations that has been updated\n",
    "The purpose of this, is to see how  many of the entries is actually updated, and whether i would make sense to only use the newest entry from all the users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_rdd_users = df_users_raw_filtered.select(\"useruuid\",\"weight\",\"height\",\"gender\",\"birthday\").rdd\\\n",
    "    .map(lambda item: (item['useruuid'],item)) \\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(check_updaste_of_user_info)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_updaste_of_user_info(items):\n",
    "    items = list(items)\n",
    "    weight_to_check = items[0][\"weight\"]\n",
    "    height_to_check = items[0][\"height\"]\n",
    "    gender_to_check = items[0][\"gender\"]\n",
    "    birtday_to_check = items[0][\"birthday\"]\n",
    "    \n",
    "    result = []\n",
    "    for item in items:\n",
    "        if item[\"weight\"] != weight_to_check:\n",
    "            result.append(1)\n",
    "        elif item[\"height\"] != height_to_check:\n",
    "            result.append(2)\n",
    "        elif item[\"gender\"] != gender_to_check:\n",
    "            result.append(3)\n",
    "        elif item[\"birthday\"] != birtday_to_check:\n",
    "            result.append(4)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"No updates: \", test_rdd_users.filter(lambda x : len(x[1]) == 0).count()\n",
    "print \"Weight updated: \",test_rdd_users.filter(lambda x : 1 in x[1] ).count()\n",
    "print \"Height updated: \", test_rdd_users.filter(lambda x : 2 in x[1]).count()\n",
    "print \"Gender updated: \", test_rdd_users.filter(lambda x : 3 in x[1]).count()\n",
    "print \"Birtday updated: \",test_rdd_users.filter(lambda x : 4 in x[1]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Motion acitvity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following combines the users total activity for each day. This information will be merged with a sleep entry based on its starttime. If the hour of the startime for the sleep entry is below 12, the day will subtracted with 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('day_key', StringType(), True),\n",
    "                     StructField('total_distance', FloatType(), True),\n",
    "                     StructField('total_steps', FloatType(), True),\n",
    "                     StructField('tota_walk', FloatType(), True),\n",
    "                     StructField('total_run', FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_motion_raw = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(pre_path+\"/motionactivity/\"+folders_to_load[11]+\"/*.avro\")).where(\"deleted_time != ''\")\\\n",
    "      .where(F.col(\"type\").isin({\"WALKING\",\"RUNNING\"}))\\\n",
    "      .select('start_time','type','distance','id','step_count','timestamp_seen','useruuid')\\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_motion_raw.select(\"distance\",\"step_count\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Types of activities: OTHER, TRANSPORTATION, WALKING, RUNNING.\n",
    "# For this study we will only look at WALKING AND RUNNING. \n",
    "# The distances is defined in mm. \n",
    "\n",
    "for folder in folders_to_load:\n",
    "    #take the activity dataset for the given month. Sort out all other types than walking and running. \n",
    "    df_motion_raw = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(pre_path+\"/motionactivity/\"+folder+\"/*.avro\")).where(\"deleted_time != ''\")\\\n",
    "      .where(F.col(\"type\").isin({\"WALKING\",\"RUNNING\"}))\\\n",
    "      .select('start_time','type','distance','id','step_count','timestamp_seen','useruuid')\\\n",
    "        \n",
    "    #Make a dataframe with the newest enty for each of the id's \n",
    "    newest_ids = df_motion_raw\\\n",
    "        .select(F.col(\"id\").alias(\"dummy_id\"),\"timestamp_seen\")\\\n",
    "        .groupBy(F.col(\"dummy_id\"))\\\n",
    "        .agg(F.max(F.col(\"timestamp_seen\")))\n",
    "        \n",
    "    print \"Newest entries found\"\n",
    "\n",
    "    #join the two dataframes\n",
    "    df_activity_no_doublets = df_motion_raw.join(newest_ids, \n",
    "                                    (df_motion_raw[\"timestamp_seen\"] == newest_ids[\"max(timestamp_seen)\"]) & \n",
    "                                    (df_motion_raw[\"id\"] == newest_ids[\"dummy_id\"]))\\\n",
    "                                    .drop(\"dummy_id\").drop(\"max(timestamp_seen)\")\\\n",
    "                                    .withColumn(\"day_key\",generate_day_key_udf(F.col(\"start_time\"),F.col(\"useruuid\")))\n",
    "            \n",
    "    df_activities_day =  df_activity_no_doublets.rdd\\\n",
    "        .map(lambda item: (item['day_key'],item)) \\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(motion_activity_mapper)\\\n",
    "        .map(lambda (key,value) : value).toDF(schema)\n",
    "\n",
    "    df_activities_day.write.format('com.databricks.spark.avro').save(bucket_path+\"activities/\"+ folder + \"_by_day\")\n",
    "    print \"Done with \", folder\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for folder in folders_to_load:\n",
    "        dd = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "          .load(bucket_path+\"activities/\"+ folder + \"_by_day\"+\"/*.avro\"))\n",
    "        print \"Loaded\"\n",
    "        dd.select(\"total_distance\",\"total_steps\",\"tota_walk\",\"total_run\").describe().show()\n",
    "        print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def motion_activity_mapper(items):\n",
    "    total_distance = 0 \n",
    "    total_steps = 0\n",
    "    tota_walk = 0\n",
    "    total_run = 0\n",
    "    key = None\n",
    "    \n",
    "    \n",
    "    for item in items:\n",
    "        key = item[\"day_key\"]\n",
    "        if item[\"distance\"] < 0:\n",
    "            continue\n",
    "        if item[\"step_count\"] < 0:\n",
    "            continue\n",
    "            \n",
    "        if item[\"type\"] == \"WALKING\":\n",
    "            tota_walk += item[\"distance\"]\n",
    "        if item[\"type\"] == \"RUNNING\":\n",
    "            total_run += item[\"distance\"]\n",
    "        \n",
    "        total_distance += item[\"distance\"]\n",
    "        for step in item[\"step_count\"]:\n",
    "            total_steps += step\n",
    "            \n",
    "    return [key,float(total_distance),float(total_steps),float(tota_walk),float(total_run)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart rate dataset contains a users heart rate at a given time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema_bpm = StructType([StructField('sleep_id', StringType(), True),\n",
    "                     StructField('bpm_during', FloatType(), True),\n",
    "                     StructField('bpm_before', FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for folder in folders_to_load:\n",
    "    print \"Loading for folder \", folder\n",
    "    df_heart_raw = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "      .load(pre_path+\"/heartrate/\"+folder+\"/*.avro\"))\\\n",
    "      .select('useruuid','time','bpm')\n",
    "    \n",
    "    #df_heart_raw_filtered.select(\"time\",F.substring(F.col(\"time\"), 1, 16).alias('time_str')).show(truncate=False)\n",
    "    \n",
    "    #Load sleep for the given period so we know which users we want heart rate from. \n",
    "    df_sleep_period = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_\"+folder+\"/*.avro\")).select(F.col(\"useruuid\").alias(\"dummy_id\")).distinct()\n",
    "\n",
    "    #Join the two dataframes so we only work with the relevant heart rates. \n",
    "    #Generate key like this heartRate#Time. Used so we only send one value to the udf and we can group all entries \n",
    "    #by user. \n",
    "    \n",
    "    df_heart_raw_filtered = df_heart_raw.join(df_sleep_period, \n",
    "                (df_heart_raw[\"useruuid\"] == df_sleep_period[\"dummy_id\"]))\\\n",
    "        .drop(\"dummy_id\")\\\n",
    "        .select(\"useruuid\",\"time\",\"bpm\", F.format_string(\"%d#%s\",F.col(\"bpm\"),F.col(\"time\")).alias(\"pbm_key\"))\n",
    "\n",
    "    #group all heart rates for each user togheter. \n",
    "    df_heart_by_user = df_heart_raw_filtered.groupby(\"useruuid\").agg(F.collect_list(\"pbm_key\"))\n",
    "\n",
    "    print \"Data filetered!!\"\n",
    "    \n",
    "    #Load the sleep entries\n",
    "    df_sleep_period_with_bpm = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "           .load(bucket_path+\"sleep_\"+folder+\"/*.avro\"))\\\n",
    "           .select(\"id\",F.col(\"useruuid\").alias(\"uid\"),\"start_time\",\"end_time\")\n",
    "    \n",
    "    \n",
    "    #Join the sleep entries with the list of heart rates. No left or right join here since we are only interested to \n",
    "    #loop through entries with bpm registered for that user. \n",
    "    df_sleep_period_with_bpm = df_sleep_period_with_bpm.join(df_heart_by_user, \n",
    "                                    (df_sleep_period_with_bpm[\"uid\"] == df_heart_by_user[\"useruuid\"]))\\\n",
    "                                    .drop(\"uid\")\n",
    "               \n",
    "    df_sleep_period_with_bpm = df_sleep_period_with_bpm.rdd.map(bpm_mapper).toDF(schema_bpm)\n",
    "            \n",
    "    df_sleep_period_with_bpm.drop(\"collect_list(pbm_key)\")\\\n",
    "        .write.format('com.databricks.spark.avro').save(bucket_path+\"bpm/\"+ \"combined_\" + folder)\n",
    "    \n",
    "    print \"Saved, done with \", folder\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bpm_mapper(item):\n",
    "    from dateutil.parser import parse\n",
    "    \n",
    "    date_end = parse(item[\"end_time\"]).replace(tzinfo=None)\n",
    "    date_start = parse(item[\"start_time\"]).replace(tzinfo=None)\n",
    "    interval_before_start = date_start - datetime.timedelta(hours=5)\n",
    "    \n",
    "    bpm_for_user = item[\"collect_list(pbm_key)\"]\n",
    "    \n",
    "    if bpm_for_user == None:\n",
    "        return item[\"id\"], None, None\n",
    "    \n",
    "    bpms_arr_before = []    \n",
    "    bpms_arr = []\n",
    "    \n",
    "    for row in bpm_for_user:\n",
    "        bpm, time_str = row.split(\"#\")\n",
    "        time = parse(time_str).replace(tzinfo=None)\n",
    "        \n",
    "        if time > date_start and time < date_end:\n",
    "            bpms_arr.append(float(bpm))\n",
    "            \n",
    "        if time > interval_before_start and time < date_start:\n",
    "            bpms_arr_before.append(float(bpm))\n",
    "            \n",
    "    bpm_during = None\n",
    "    bpm_before = None\n",
    "\n",
    "    if len(bpms_arr) > 0:\n",
    "        bpm_during = sum(bpms_arr) / float(len(bpms_arr))\n",
    "    if len(bpms_arr_before) > 0:\n",
    "        bpm_before = sum(bpms_arr_before) / float(len(bpms_arr_before))\n",
    "    \n",
    "    return item[\"id\"], bpm_during , bpm_before "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating final datasets\n",
    "This section combines all the sub datasets which has been generated throughout this notebook to the following datasets:\n",
    "<ul>\n",
    "<li>Dataset were sleep entries is the rows. Additional metadata has been added such as forexample location, activity, app usage, ect..\n",
    "</li>\n",
    "<li>Dataset where the unique user is the row. Each row will contain information such as age, gender, nationality, chronotype, social jetlag, etc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_user_nationality = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"user_nationality\")) \n",
    "print df_user_nationality.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nation_count = df_user_nationality.select(\"country\").groupBy(\"country\").count().orderBy(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the users dataset, and generate a new row id which corresponds to the row. \n",
    "#This is the new user id from now on so the dataset can be take home also. \n",
    "df_users = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"users\"))\\\n",
    "                .withColumn('bmi', udf_bmi_of_user(F.col('weight'),F.col(\"height\")))\\\n",
    "                #.withColumn(\"user_id\", F.monotonically_increasing_id())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a df with the new and the old id, \n",
    "#which can be used to join with the other datasets and then remove the old id column\n",
    "\n",
    "#df_user_new_id = df_users.select(F.col(\"useruuid\").alias(\"old_id\"),\"user_id\").cache()\n",
    "#df_user_new_id.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"user_new_ids\")\n",
    "\n",
    "df_user_new_id = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"final/\"+ \"user_new_ids\")).cache()\n",
    "\n",
    "df_users = df_users.join(df_user_new_id, \n",
    "            (df_users[\"useruuid\"] == df_user_new_id[\"old_id\"]))\\\n",
    "            .drop(\"useruuid\").drop(\"old_id\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The dataset which contains information about, social jetlag, chronotype etc...\n",
    "df_users_sleep_stats = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_stats_total\"))\n",
    "\n",
    "df_users_sleep_stats = df_users_sleep_stats.join(df_user_new_id, \n",
    "            (df_users_sleep_stats[\"useruuid\"] == df_user_new_id[\"old_id\"]))\\\n",
    "           .drop(\"useruuid\").drop(\"old_id\").select(\"*\",F.col(\"user_id\").alias(\"uid\")).drop(\"user_id\")\n",
    "    \n",
    "print df_users_sleep_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_users.select(\"height\",\"weight\",\"age\",\"bmi\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New sleep dataset with extra metadata\n",
    "What is missing: \n",
    "<ul>\n",
    "    <li>Light polution</li>\n",
    "</ul>\n",
    "\n",
    "What will not be included:\n",
    "<ul>\n",
    "    <li>Babies</li>\n",
    "    <li>BPM</li>\n",
    "</ul>\n",
    "\n",
    "Light polution, alarm?, Babies?, Screen recency, chronotype(Average - average by month), BPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All sleep entries\n",
    "load_all_files_sleep_arr = []\n",
    "for folder in folders_to_load: \n",
    "    load_all_files_sleep_arr.append(bucket_path+\"sleep_\"+folder+\"/*.avro\")\n",
    "    \n",
    "#All sleep entries with location. \n",
    "load_all_files_location_arr = []\n",
    "for folder in folders_to_load: \n",
    "    load_all_files_location_arr.append(bucket_path+\"sleep_loc_filled_\"+folder+\"/*.avro\")\n",
    "    \n",
    "#All app bins for one hour\n",
    "load_all_files_app_sleep_arr = []\n",
    "for folder in folders_to_load: \n",
    "    load_all_files_app_sleep_arr.append(bucket_path+\"app_usage/app_bins_one_\"+folder+\"/*.avro\")\n",
    "    \n",
    "#All app bins for two and three hours. \n",
    "load_all_files_app_sleep_rest_arr = []\n",
    "for folder in folders_to_load: \n",
    "    load_all_files_app_sleep_rest_arr.append(bucket_path+\"app_usage/app_bins_two_three_\"+folder+\"/*.avro\")\n",
    "    \n",
    "#App recency and is wake up by alarm\n",
    "load_all_recency = []\n",
    "for folder in folders_to_load:\n",
    "    load_all_recency.append(bucket_path+\"/app_usage/app_usage_before_bed_\"+folder+\"/*.avro\")\n",
    "        \n",
    "#All physical activities by day.\n",
    "load_all_activities_by_date = []\n",
    "for folder in folders_to_load:\n",
    "    load_all_activities_by_date.append(bucket_path+\"activities/\"+folder+\"_by_day/*.avro\")\n",
    "    \n",
    "#Load bpm all sleep entries\n",
    "load_bpm_arr = []\n",
    "for folder in folders_to_load:\n",
    "    load_bpm_arr.append(bucket_path+\"bpm/\"+ folder+\"/*.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load all sleep entries\n",
    "sleeps_total_df = (sqlContext.read.format(\"com.databricks.spark.avro\").load(load_all_files_sleep_arr))\\\n",
    "    .withColumn('sleep_bin', time_bin_udf(F.col('start_time'), F.col(\"useruuid\")))\\\n",
    "    .withColumn(\"day_key_sleep\",generate_day_key_adjusted_udf(F.col(\"start_time\"),F.col(\"useruuid\")))\\\n",
    "    .drop(\"week_number_key_user\").drop(\"end_time_dt\").drop(\"end_time_dt_utc\")\\\n",
    "    .drop(\"start_time_dt\").drop(\"start_time_dt_utc\")\\\n",
    "    .select(\"*\",F.weekofyear(F.col(\"start_time\")).alias(\"weeknumber\"))\n",
    "    \n",
    "\n",
    "sleeps_total_df = sleeps_total_df.join(df_user_new_id, \n",
    "            (sleeps_total_df[\"useruuid\"] == df_user_new_id[\"old_id\"]))\\\n",
    "           .drop(\"useruuid\").drop(\"old_id\")\n",
    "\n",
    "print sleeps_total_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the populations from dropbox. \n",
    "transferData.get_file(\"/population.p\",\"population.p\")\n",
    "populations = pickle.load(open(\"population.p\",\"rb\"))\n",
    "populations_bc = sc.broadcast(populations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def population_by_loc(lon,lat):\n",
    "    key = str(lon) + \"-\" + str(lat) + \"\\n\" #stupid mistake, so add \\n to the end...\n",
    "    if key in populations_bc.value:\n",
    "        value = populations_bc.value[key]\n",
    "\n",
    "        if value == None:\n",
    "            return None\n",
    "        else:\n",
    "            return float(value)\n",
    "    \n",
    "    return None\n",
    "\n",
    "population_by_loc_udf = F.udf(population_by_loc,FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lon: float (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- altitude: float (nullable = true)\n",
      " |-- daylight: float (nullable = true)\n",
      " |-- location_sleep_id: string (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Get all locations and join with the sleep dataset\n",
    "loc_total_df = (sqlContext.read.format(\"com.databricks.spark.avro\").load(load_all_files_location_arr))\\\n",
    "    .select(\"lon\",\"lat\",\"country\",\"region\",\"altitude\",\n",
    "           F.col(\"sunlight\").alias(\"daylight\"),F.col(\"id\").alias(\"location_sleep_id\"))\\\n",
    "    .withColumn('population', population_by_loc_udf(F.col('lon'),F.col('lat')))\n",
    "\n",
    "print loc_total_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeps_total_df = sleeps_total_df.join(loc_total_df, \n",
    "            (sleeps_total_df[\"id\"] == loc_total_df[\"location_sleep_id\"]),\"left_outer\")\\\n",
    "           .drop(\"location_sleep_id\")\n",
    "    \n",
    "#print \"Entries with no location \", sleeps_total_df.where(F.col(\"lat\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeps_total_df.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"sleep_loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_meta = df_users.select(F.col(\"useruuid\"), \"age\",\"gender\",\"bmi\",\"weight\",\"height\")\n",
    "\n",
    "df_user_meta = df_user_meta.join(df_user_new_id, \n",
    "            (df_user_meta[\"useruuid\"] == df_user_new_id[\"old_id\"]))\\\n",
    "           .drop(\"useruuid\").drop(\"old_id\").select(\"*\",F.col(\"user_id\").alias(\"uid\")).drop(\"user_id\")\n",
    "\n",
    "sleeps_total_df = sleeps_total_df.join(df_user_meta, \n",
    "                (df_user_meta[\"uid\"] == sleeps_total_df[\"user_id\"])).drop(\"uid\").drop(\"old_id\")\n",
    "\n",
    "#print \"Entries with no user information\", sleeps_total_df.where(F.col(\"gender\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeps_total_df.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"sleep_loc_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sleep stats \n",
    "sleeps_total_df = sleeps_total_df.join(df_users_sleep_stats, \n",
    "            (sleeps_total_df[\"user_id\"] == df_users_sleep_stats[\"uid\"]),\"left_outer\")\\\n",
    "            .drop(\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sleeps_total_df.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"sleep_loc_user_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sleeps_total_df = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"final/\"+ \"sleep_loc_user_stats_one_rec_ac\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_app = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(load_all_files_app_sleep_arr))\n",
    "\n",
    "sleeps_total_df = sleeps_total_df.join(df_total_app, \n",
    "            (sleeps_total_df[\"sleep_bin\"] == df_total_app[\"timebin\"]),\"left_outer\")\\\n",
    "            .drop(\"timebin\")\n",
    "    \n",
    "\n",
    "#.where(F.col(\"seconds_one\").isNotNull())\n",
    "#print \"Entries with no app info one hour\", sleeps_total_df.where(F.col(\"seconds_one\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sleeps_total_df.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"sleep_loc_user_stats_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_total_app = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(load_all_files_app_sleep_rest_arr))\n",
    "\n",
    "sleeps_total_df = sleeps_total_df.join(df_total_app, \n",
    "            (sleeps_total_df[\"sleep_bin\"] == df_total_app[\"timebin\"]),\"left_outer\")\\\n",
    "            .drop(\"timebin\")\n",
    "    \n",
    "#print \"Entries with no app info two hours\", sleeps_total_df.where(F.col(\"seconds_two\").isNull()).count()\n",
    "#print \"Entries with no app info three hours\", sleeps_total_df.where(F.col(\"seconds_three\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeps_total_df.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"sleep_loc_user_stats_one_rec_ac_rest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sleep_id: string (nullable = true)\n",
      " |-- alarm_used: boolean (nullable = true)\n",
      " |-- alarm_app: string (nullable = true)\n",
      " |-- last_app_time_since: float (nullable = true)\n",
      " |-- last_app_date: string (nullable = true)\n",
      " |-- last_app_category: string (nullable = true)\n",
      " |-- last_app_name: string (nullable = true)\n",
      " |-- first_app_time_since: float (nullable = true)\n",
      " |-- first_app_date: string (nullable = true)\n",
      " |-- first_app_category: string (nullable = true)\n",
      " |-- first_app_name: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#App recency.. \n",
    "app_recency_total = (sqlContext.read.format(\"com.databricks.spark.avro\").load(load_all_recency))\n",
    "print app_recency_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sleeps_total_df = sleeps_total_df.join(app_recency_total, \n",
    "            (sleeps_total_df[\"id\"] == app_recency_total[\"sleep_id\"]),\"left_outer\")\\\n",
    "           .drop(\"sleep_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sleeps_total_df.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"sleep_loc_user_stats_one_rec_ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Physiacal activity\n",
    "phy_ac_total = (sqlContext.read.format(\"com.databricks.spark.avro\").load(load_all_activities_by_date))\n",
    "\n",
    "sleeps_total_df = sleeps_total_df.join(phy_ac_total, \n",
    "            (sleeps_total_df[\"day_key_sleep\"] == phy_ac_total[\"day_key\"]),\"left_outer\")\\\n",
    "            .drop(\"day_key\")\n",
    "#print \"Entries with no activity\", sleeps_total_df.where(F.col(\"seconds_two\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeps_total_df.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"sleep_loc_user_stats_one_ac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New User dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the features we want for each of the rows:\n",
    "<ul>\n",
    "    <li>**Country**</li>\n",
    "    <li>Region</li>\n",
    "    <li>Population density (Average)</li>\n",
    "    <li>Screen exposure: Recency (Average)</li>\n",
    "    <li>Screen exposure: Amount (Average)</li>\n",
    "    <li>Physical activity (Average)</li>\n",
    "    <li>**Age**</li>\n",
    "    <li>**Gender**</li>\n",
    "    <li>**BMI**</li>\n",
    "    <li>**Weight**</li>\n",
    "    <li>**Height**</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sleep_stats_total = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "               .load(bucket_path+\"sleep_stats_total\"))\n",
    "\n",
    "\n",
    "df_sleep_stats_total = df_sleep_stats_total.join(df_user_nationality, \n",
    "            (df_sleep_stats_total[\"useruuid\"] == df_user_nationality[\"user\"])).drop(\"user\")\n",
    "\n",
    "print df_sleep_stats_total.printSchema()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sleep_stats_total = df_sleep_stats_total.join(df_user_new_id, \n",
    "            (df_sleep_stats_total[\"useruuid\"] == df_user_new_id[\"old_id\"]))\\\n",
    "           .drop(\"old_id\").select(\"*\",F.col(\"user_id\").alias(\"uid\")).drop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_user_meta = df_users.select(F.col(\"user_id\"), \"age\",\"gender\",\"bmi\",\"weight\",\"height\")\n",
    "\n",
    "df_sleep_stats_total = df_sleep_stats_total.join(df_user_meta, \n",
    "                (df_sleep_stats_total[\"uid\"] == df_user_meta[\"user_id\"])).drop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sleep_stats_total.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"stat_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sleep_stats_total = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "               .load(bucket_path+\"final/\"+ \"stat_user\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_last = sleeps_total_df.select(\"user_id\",\"last_app_time_since\").groupBy(\"user_id\").agg(F.avg(\"last_app_time_since\"))\n",
    "total_amount = sleeps_total_df.select(\"user_id\",\"seconds_one\").groupBy(\"user_id\").agg(F.avg(\"seconds_one\"))\n",
    "total_distance = sleeps_total_df.select(\"user_id\",\"total_distance\").groupBy(\"user_id\").agg(F.avg(\"total_distance\"))\n",
    "total_population = sleeps_total_df.select(\"user_id\",\"population\").groupBy(\"user_id\").agg(F.avg(\"population\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_stats_total = df_sleep_stats_total.join(total_last, \n",
    "                (df_sleep_stats_total[\"uid\"] == total_last[\"user_id\"])).drop(\"user_id\")\n",
    "\n",
    "df_sleep_stats_total = df_sleep_stats_total.join(total_amount, \n",
    "                (df_sleep_stats_total[\"uid\"] == total_amount[\"user_id\"])).drop(\"user_id\")\n",
    "\n",
    "df_sleep_stats_total = df_sleep_stats_total.join(total_distance, \n",
    "                (df_sleep_stats_total[\"uid\"] == total_distance[\"user_id\"])).drop(\"user_id\")\n",
    "\n",
    "df_sleep_stats_total = df_sleep_stats_total.join(total_population, \n",
    "                (df_sleep_stats_total[\"uid\"] == total_population[\"user_id\"])).drop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_sleep_stats_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sleep_stats_total = df_sleep_stats_total.select(\"*\",\n",
    "                                                   F.col(\"avg(last_app_time_since)\").alias(\"avg_last_app_time_since\"),\n",
    "                                                   F.col(\"avg(seconds_one)\").alias(\"avg_seconds_one\"),\n",
    "                                                   F.col(\"avg(total_distance)\").alias(\"avg_total_distance\"),\n",
    "                                                   F.col(\"avg(population)\").alias(\"avg_population\")\n",
    "                                                  ).drop(\"avg(last_app_time_since)\").drop(\"avg(seconds_one)\")\\\n",
    "                                                    .drop(\"avg(total_distance)\").drop(\"avg(population)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_stats_total.write.format('com.databricks.spark.avro').save(bucket_path+\"final/\"+ \"stat_user_extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_user_meta = df_users.select(F.col(\"lifelog_useruuid\").alias(\"user\"), \"age\",\"gender\",\"bmi\",\"weight\",\"height\")\n",
    "\n",
    "for folder in folders_to_load:\n",
    "    df_sleep_stats_period = (sqlContext.read.format(\"com.databricks.spark.avro\")\\\n",
    "                .load(bucket_path+\"sleep_stats_\"+folder+\"/*.avro\")) \n",
    "    \n",
    "    df_sleep_stats_period = df_sleep_stats_period.join(df_user_meta, \n",
    "                (df_sleep_stats_period[\"useruuid\"] == df_user_meta[\"user\"])).drop(\"user\")\n",
    "    \n",
    "    df_sleep_stats_period = df_sleep_stats_period.join(df_user_nationality, \n",
    "            (df_sleep_stats_period[\"useruuid\"] == df_user_nationality[\"user\"])).drop(\"user\")\n",
    "\n",
    "    df_sleep_stats_period = df_sleep_stats_period.join(df_user_new_id, \n",
    "            (df_sleep_stats_period[\"useruuid\"] == df_user_new_id[\"old_id\"]))\\\n",
    "           .drop(\"old_id\").select(\"*\",F.col(\"user_id\").alias(\"uid\")).drop(\"user_id\")\n",
    "    \n",
    "    df_sleep_stats_period.write.format('com.databricks.spark.avro').save(\n",
    "        bucket_path+\"final/\"+ \"sleep_stats_\"+folder)\n",
    "    \n",
    "    print \"Done with\", folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# UDF\n",
    "\n",
    "The following is the user defined functions which is used in this project. It has been tried to keep the number of UDF's at a minimum since there are very performance heavy compared to native spark sql functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_difference_time(end_str, start_str):\n",
    "    from dateutil.parser import parse\n",
    "    \n",
    "    if start_str != None and end_str != None:\n",
    "        date_end = parse(end_str).replace(tzinfo=None)\n",
    "        date_start = parse(start_str).replace(tzinfo=None)\n",
    "    \n",
    "        delta = (date_end-date_start).total_seconds()\n",
    "    \n",
    "        return float(delta)\n",
    "    \n",
    "    return None\n",
    "        \n",
    "def generate_day_key_adjusted(date_str,useruuid):\n",
    "    from dateutil.parser import parse\n",
    "    date = parse(date_str)\n",
    "    unaware_date = date.replace(tzinfo=None)\n",
    "    if unaware_date.hour < 12:\n",
    "        unaware_date = unaware_date - datetime.timedelta(days=1)\n",
    "\n",
    "    key = str(unaware_date.year) + \"-\" + str(unaware_date.month) + \"-\" + str(unaware_date.day) + \"_\" + useruuid\n",
    "    return key\n",
    "\n",
    "def generate_day_key(date_str,useruuid):\n",
    "    from dateutil.parser import parse\n",
    "    date = parse(date_str)\n",
    "    unaware_date = date.replace(tzinfo=None)\n",
    "\n",
    "    key = str(unaware_date.year) + \"-\" + str(unaware_date.month) + \"-\" + str(unaware_date.day) + \"_\" + useruuid\n",
    "    return key\n",
    "\n",
    "def generate_time_bin(date_str, useruuid):\n",
    "    \n",
    "    from dateutil.parser import parse\n",
    "    date = parse(date_str)\n",
    "    date_utc = date - date.utcoffset()\n",
    "    unaware_date = date_utc.replace(tzinfo=None)\n",
    "\n",
    "    minute = int(5 * round(float(unaware_date.minute)/5))\n",
    "    diff = minute - unaware_date.minute\n",
    "    time = unaware_date + datetime.timedelta(minutes=diff)\n",
    "\n",
    "    key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute) + \"_\" + useruuid\n",
    "    return key\n",
    "\n",
    "def generate_time_bin_minute(date_str, useruuid):\n",
    "    \n",
    "    from dateutil.parser import parse\n",
    "    date = parse(date_str)\n",
    "    date_utc = date - date.utcoffset()\n",
    "    unaware_date = date_utc.replace(tzinfo=None)\n",
    "\n",
    "    minute = int(1 * round(float(unaware_date.minute)/1))\n",
    "    diff = minute - unaware_date.minute\n",
    "    time = unaware_date + datetime.timedelta(minutes=diff)\n",
    "\n",
    "    key = str(time.year) + \"-\" + str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute) + \"_\" + useruuid\n",
    "    return key\n",
    "\n",
    "def func_get_device_type(devices):\n",
    "    return devices[0][\"name\"]\n",
    "\n",
    "def calculate_age_of_user(x):\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        born = parse(x)\n",
    "        return (datetime.today() - born).days/365\n",
    "    except Exception:\n",
    "        return 0\n",
    "    \n",
    "def calculate_BMI(weight,height):\n",
    "    #Weight in  kg, height in meters\n",
    "    height_m = float(height/float(1000))\n",
    "    weight_kg = weight/1000\n",
    "    bmi = weight_kg/(height_m*height_m)\n",
    "\n",
    "    if bmi <= 18.5:\n",
    "        return 0\n",
    "        #print('Your BMI is', bmi,'which means you are underweight.')\n",
    "    elif bmi > 18.5 and bmi < 25:\n",
    "        return 1\n",
    "        #print('Your BMI is', bmi,'which means you are normal.')\n",
    "    elif bmi > 25 and bmi < 30:\n",
    "        return 2\n",
    "        #print('your BMI is', bmi,'overweight.')\n",
    "    elif bmi > 30:\n",
    "        return 3\n",
    "        #print('Your BMI is', bmi,'which means you are obese.')\n",
    "\n",
    "\n",
    "udf_age_of_user = F.udf(calculate_age_of_user, IntegerType())\n",
    "udf_bmi_of_user = F.udf(calculate_BMI, IntegerType())\n",
    "\n",
    "find_difference_time_udf = F.udf(find_difference_time,FloatType())\n",
    "func_get_device_type_udf = F.udf(func_get_device_type,StringType())\n",
    "time_bin_udf = F.udf(generate_time_bin, StringType())\n",
    "time_bin_minute_udf = F.udf(generate_time_bin_minute, StringType())\n",
    "generate_day_key_udf = F.udf(generate_day_key,StringType())\n",
    "generate_day_key_adjusted_udf = F.udf(generate_day_key_adjusted,StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daylight code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing place.py\n"
     ]
    }
   ],
   "source": [
    "%%file place.py\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "from sun import Sun\n",
    "\n",
    "class Place(object):\n",
    "    def __init__(self, name, coords):\n",
    "        self.name = name        # string\n",
    "        self.coords = coords    # tuple (E/W long, N/S lat)\n",
    "\n",
    "def _hoursmins(hours):\n",
    "    \"\"\"Convert floating point decimal time in hours to integer hrs,mins\"\"\"\n",
    "    frac,h = math.modf(hours)\n",
    "    m = round(frac*60, 0)\n",
    "    if m == 60: # rounded up to next hour\n",
    "        h += 1; m = 0\n",
    "    return int(h),int(m)\n",
    "\n",
    "def _ymd(date):\n",
    "    \"\"\"Return y,m,d from datetime object as tuple\"\"\"\n",
    "    return date.timetuple()[:3]\n",
    "\n",
    "def getsuninfo(location, date=None):\n",
    "    \"\"\"Return utc datetime of sunrise, sunset, and length of day in hrs,mins)\"\"\"\n",
    "    if date == None:\n",
    "        querydate = datetime.date.today()\n",
    "    else: # date given should be datetime instance\n",
    "        querydate = date\n",
    "\n",
    "    args = _ymd(querydate) + location.coords\n",
    "    utcrise, utcset = Sun().sunRiseSet(*args)\n",
    "    daylength = Sun().dayLength(*args)\n",
    "    hrs,mins = _hoursmins(daylength)\n",
    "\n",
    "    risehour, risemin = _hoursmins(utcrise)\n",
    "    sethour, setmin   = _hoursmins(utcset)\n",
    "\n",
    "    # convert times to timedelta values (ie from midnight utc of the date)\n",
    "    midnight = datetime.datetime(*_ymd(querydate))\n",
    "    deltarise = datetime.timedelta(hours=risehour, minutes=risemin)\n",
    "    utcdatetimerise = midnight+deltarise\n",
    "    deltaset = datetime.timedelta(hours=sethour, minutes=setmin)\n",
    "    utcdatetimeset  = midnight+deltaset\n",
    "\n",
    "    # convert results from UTC time to local time of location\n",
    "    #localrise = utcdatetimerise.astimezone(location.tz)\n",
    "    #localset  = utcdatetimeset.astimezone(location.tz)\n",
    "\n",
    "    return utcdatetimerise, utcdatetimeset, hrs, mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sun.py\n"
     ]
    }
   ],
   "source": [
    "%%file sun.py\n",
    "# -*- coding: iso-8859-1 -*-\n",
    "SUN_PY_VERSION = 1.5\n",
    " \n",
    "import math\n",
    "from math import pi\n",
    " \n",
    "import calendar\n",
    " \n",
    "class Sun:\n",
    "   \n",
    "    def __init__(self):\n",
    "        \"\"\"\"\"\"\n",
    "       \n",
    "        # Some conversion factors between radians and degrees\n",
    "        self.RADEG = 180.0 / pi\n",
    "        self.DEGRAD = pi / 180.0\n",
    "        self.INV360 = 1.0 / 360.0\n",
    "       \n",
    " \n",
    "    def daysSince2000Jan0(self, y, m, d):\n",
    "        \"\"\"A macro to compute the number of days elapsed since 2000 Jan 0.0\n",
    "           (which is equal to 1999 Dec 31, 0h UT)\"\"\"\n",
    "        return (367*(y)-((7*((y)+(((m)+9)/12)))/4)+((275*(m))/9)+(d)-730530)\n",
    "   \n",
    " \n",
    "    # The trigonometric functions in degrees\n",
    "    def sind(self, x):\n",
    "        \"\"\"Returns the sin in degrees\"\"\"\n",
    "        return math.sin(x * self.DEGRAD)\n",
    " \n",
    "    def cosd(self, x):\n",
    "        \"\"\"Returns the cos in degrees\"\"\"\n",
    "        return math.cos(x * self.DEGRAD)\n",
    " \n",
    "    def tand(self, x):\n",
    "        \"\"\"Returns the tan in degrees\"\"\"\n",
    "        return math.tan(x * self.DEGRAD)\n",
    " \n",
    "    def atand(self, x):\n",
    "        \"\"\"Returns the arc tan in degrees\"\"\"\n",
    "        return math.atan(x) * self.RADEG\n",
    "   \n",
    "    def asind(self, x):\n",
    "        \"\"\"Returns the arc sin in degrees\"\"\"\n",
    "        return math.asin(x) * self.RADEG\n",
    " \n",
    "    def acosd(self, x):\n",
    "        \"\"\"Returns the arc cos in degrees\"\"\"\n",
    "        return math.acos(x) * self.RADEG\n",
    " \n",
    "    def atan2d(self, y, x):\n",
    "        \"\"\"Returns the atan2 in degrees\"\"\"\n",
    "        return math.atan2(y, x) * self.RADEG\n",
    " \n",
    "    # Following are some macros around the \"workhorse\" function __daylen__\n",
    "    # They mainly fill in the desired values for the reference altitude    \n",
    "    # below the horizon, and also selects whether this altitude should    \n",
    "    # refer to the Sun's center or its upper limb.                        \n",
    " \n",
    "    def dayLength(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes the length of the day, from sunrise to sunset.\n",
    "        Sunrise/set is considered to occur when the Sun's upper limb is\n",
    "        35 arc minutes below the horizon (this accounts for the refraction\n",
    "        of the Earth's atmosphere).\n",
    "        \"\"\"\n",
    "        return self.__daylen__(year, month, day, lon, lat, -35.0/60.0, 1)\n",
    "   \n",
    "    def dayCivilTwilightLength(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes the length of the day, including civil twilight.\n",
    "        Civil twilight starts/ends when the Sun's center is 6 degrees below\n",
    "        the horizon.\n",
    "        \"\"\"\n",
    "        return self.__daylen__(year, month, day, lon, lat, -6.0, 0)\n",
    " \n",
    "    def dayNauticalTwilightLength(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes the length of the day, incl. nautical twilight.\n",
    "        Nautical twilight starts/ends when the Sun's center is 12 degrees\n",
    "        below the horizon.\n",
    "        \"\"\"\n",
    "        return self.__daylen__(year, month, day, lon, lat, -12.0, 0)\n",
    " \n",
    "    def dayAstronomicalTwilightLength(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes the length of the day, incl. astronomical twilight.\n",
    "        Astronomical twilight starts/ends when the Sun's center is 18 degrees\n",
    "        below the horizon.\n",
    "        \"\"\"\n",
    "        return self.__daylen__(year, month, day, lon, lat, -18.0, 0)\n",
    "   \n",
    "    def sunRiseSet(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes times for sunrise/sunset.\n",
    "        Sunrise/set is considered to occur when the Sun's upper limb is\n",
    "        35 arc minutes below the horizon (this accounts for the refraction\n",
    "        of the Earth's atmosphere).\n",
    "        \"\"\"\n",
    "        return self.__sunriset__(year, month, day, lon, lat, -35.0/60.0, 1)\n",
    " \n",
    "   \n",
    "    def civilTwilight(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes the start and end times of civil twilight.\n",
    "        Civil twilight starts/ends when the Sun's center is 6 degrees below\n",
    "        the horizon.\n",
    "        \"\"\"\n",
    "        return self.__sunriset__(year, month, day, lon, lat, -6.0, 0)\n",
    "   \n",
    "    def nauticalTwilight(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes the start and end times of nautical twilight.\n",
    "        Nautical twilight starts/ends when the Sun's center is 12 degrees\n",
    "        below the horizon.\n",
    "        \"\"\"\n",
    "        return self.__sunriset__(year, month, day, lon, lat, -12.0, 0)\n",
    "   \n",
    "    def astronomicalTwilight(self, year, month, day, lon, lat):\n",
    "        \"\"\"\n",
    "        This macro computes the start and end times of astronomical twilight.\n",
    "        Astronomical twilight starts/ends when the Sun's center is 18 degrees\n",
    "        below the horizon.\n",
    "        \"\"\"\n",
    "        return self.__sunriset__(year, month, day, lon, lat, -18.0, 0)\n",
    "   \n",
    "    # The \"workhorse\" function for sun rise/set times\n",
    "    def __sunriset__(self, year, month, day, lon, lat, altit, upper_limb):\n",
    "        \"\"\"\n",
    "        Note: year,month,date = calendar date, 1801-2099 only.\n",
    "              Eastern longitude positive, Western longitude negative\n",
    "             Northern latitude positive, Southern latitude negative\n",
    "              The longitude value IS critical in this function!\n",
    "              altit = the altitude which the Sun should cross\n",
    "                      Set to -35/60 degrees for rise/set, -6 degrees\n",
    "                      for civil, -12 degrees for nautical and -18\n",
    "                      degrees for astronomical twilight.\n",
    "                upper_limb: non-zero -> upper limb, zero -> center\n",
    "                      Set to non-zero (e.g. 1) when computing rise/set\n",
    "                      times, and to zero when computing start/end of\n",
    "                      twilight.\n",
    "              *rise = where to store the rise time\n",
    "              *set  = where to store the set  time\n",
    "                      Both times are relative to the specified altitude,\n",
    "                      and thus this function can be used to compute\n",
    "                      various twilight times, as well as rise/set times\n",
    "        Return value:  0 = sun rises/sets this day, times stored at\n",
    "                          *trise and *tset.\n",
    "                      +1 = sun above the specified 'horizon' 24 hours.\n",
    "                           *trise set to time when the sun is at south,\n",
    "                           minus 12 hours while *tset is set to the south\n",
    "                           time plus 12 hours. 'Day' length = 24 hours\n",
    "                      -1 = sun is below the specified 'horizon' 24 hours\n",
    "                           'Day' length = 0 hours, *trise and *tset are\n",
    "                            both set to the time when the sun is at south.\n",
    "        \"\"\"\n",
    "        # Compute d of 12h local mean solar time\n",
    "        d = self.daysSince2000Jan0(year,month,day) + 0.5 - (lon/360.0)\n",
    "       \n",
    "        # Compute local sidereal time of this moment\n",
    "        sidtime = self.revolution(self.GMST0(d) + 180.0 + lon)\n",
    "       \n",
    "        # Compute Sun's RA + Decl at this moment\n",
    "        res = self.sunRADec(d)\n",
    "        sRA = res[0]\n",
    "        sdec = res[1]\n",
    "        sr = res[2]\n",
    "       \n",
    "        # Compute time when Sun is at south - in hours UT\n",
    "        tsouth = 12.0 - self.rev180(sidtime - sRA)/15.0;\n",
    "       \n",
    "        # Compute the Sun's apparent radius, degrees\n",
    "        sradius = 0.2666 / sr;\n",
    "       \n",
    "        # Do correction to upper limb, if necessary\n",
    "        if upper_limb:\n",
    "            altit = altit - sradius\n",
    "       \n",
    "        # Compute the diurnal arc that the Sun traverses to reach\n",
    "        # the specified altitude altit:\n",
    "       \n",
    "        cost = (self.sind(altit) - self.sind(lat) * self.sind(sdec))/\\\n",
    "               (self.cosd(lat) * self.cosd(sdec))\n",
    " \n",
    "        if cost >= 1.0:\n",
    "            rc = -1\n",
    "            t = 0.0           # Sun always below altit\n",
    "           \n",
    "        elif cost <= -1.0:\n",
    "            rc = +1\n",
    "            t = 12.0;         # Sun always above altit\n",
    " \n",
    "        else:\n",
    "            t = self.acosd(cost)/15.0   # The diurnal arc, hours\n",
    " \n",
    "       \n",
    "        # Store rise and set times - in hours UT\n",
    "        return (tsouth-t, tsouth+t)\n",
    " \n",
    " \n",
    "    def __daylen__(self, year, month, day, lon, lat, altit, upper_limb):\n",
    "        \"\"\"\n",
    "        Note: year,month,date = calendar date, 1801-2099 only.            \n",
    "              Eastern longitude positive, Western longitude negative      \n",
    "              Northern latitude positive, Southern latitude negative      \n",
    "              The longitude value is not critical. Set it to the correct  \n",
    "              longitude if you're picky, otherwise set to, say, 0.0    \n",
    "              The latitude however IS critical - be sure to get it correct\n",
    "              altit = the altitude which the Sun should cross              \n",
    "                      Set to -35/60 degrees for rise/set, -6 degrees      \n",
    "                      for civil, -12 degrees for nautical and -18          \n",
    "                      degrees for astronomical twilight.                  \n",
    "                upper_limb: non-zero -> upper limb, zero -> center        \n",
    "                      Set to non-zero (e.g. 1) when computing day length  \n",
    "                      and to zero when computing day+twilight length.      \n",
    "                                                       \n",
    "        \"\"\"\n",
    "       \n",
    "        # Compute d of 12h local mean solar time\n",
    "        d = self.daysSince2000Jan0(year,month,day) + 0.5 - (lon/360.0)\n",
    "                 \n",
    "        # Compute obliquity of ecliptic (inclination of Earth's axis)\n",
    "        obl_ecl = 23.4393 - 3.563E-7 * d\n",
    "       \n",
    "        # Compute Sun's position\n",
    "        res = self.sunpos(d)\n",
    "        slon = res[0]\n",
    "        sr = res[1]\n",
    "       \n",
    "        # Compute sine and cosine of Sun's declination\n",
    "        sin_sdecl = self.sind(obl_ecl) * self.sind(slon)\n",
    "        cos_sdecl = math.sqrt(1.0 - sin_sdecl * sin_sdecl)\n",
    "       \n",
    "        # Compute the Sun's apparent radius, degrees\n",
    "        sradius = 0.2666 / sr\n",
    "       \n",
    "        # Do correction to upper limb, if necessary\n",
    "        if upper_limb:\n",
    "            altit = altit - sradius\n",
    " \n",
    "           \n",
    "        cost = (self.sind(altit) - self.sind(lat) * sin_sdecl) / \\\n",
    "               (self.cosd(lat) * cos_sdecl)\n",
    "        if cost >= 1.0:\n",
    "            t = 0.0             # Sun always below altit\n",
    "       \n",
    "        elif cost <= -1.0:\n",
    "            t = 24.0      # Sun always above altit\n",
    "       \n",
    "        else:\n",
    "            t = (2.0/15.0) * self.acosd(cost);     # The diurnal arc, hours\n",
    "           \n",
    "        return t\n",
    " \n",
    "   \n",
    "    def sunpos(self, d):\n",
    "        \"\"\"\n",
    "        Computes the Sun's ecliptic longitude and distance\n",
    "        at an instant given in d, number of days since    \n",
    "        2000 Jan 0.0.  The Sun's ecliptic latitude is not  \n",
    "        computed, since it's always very near 0.          \n",
    "        \"\"\"\n",
    " \n",
    "        # Compute mean elements\n",
    "        M = self.revolution(356.0470 + 0.9856002585 * d)\n",
    "        w = 282.9404 + 4.70935E-5 * d\n",
    "        e = 0.016709 - 1.151E-9 * d\n",
    "       \n",
    "        # Compute true longitude and radius vector\n",
    "        E = M + e * self.RADEG * self.sind(M) * (1.0 + e * self.cosd(M))\n",
    "        x = self.cosd(E) - e\n",
    "        y = math.sqrt(1.0 - e*e) * self.sind(E)\n",
    "        r = math.sqrt(x*x + y*y)              #Solar distance\n",
    "        v = self.atan2d(y, x)                 # True anomaly\n",
    "        lon = v + w                        # True solar longitude\n",
    "        if lon >= 360.0:\n",
    "            lon = lon - 360.0   # Make it 0..360 degrees\n",
    "           \n",
    "        return (lon,r)\n",
    "   \n",
    " \n",
    "    def sunRADec(self, d):\n",
    "        \"\"\"\n",
    "       Returns the angle of the Sun (RA)\n",
    "       the declination (dec) and the distance of the Sun (r)\n",
    "       for a given day d.\n",
    "       \"\"\"\n",
    "       \n",
    "        # Compute Sun's ecliptical coordinates\n",
    "        res = self.sunpos(d)\n",
    "        lon = res[0]  # True solar longitude\n",
    "        r = res[1]    # Solar distance\n",
    "       \n",
    "        # Compute ecliptic rectangular coordinates (z=0)\n",
    "        x = r * self.cosd(lon)\n",
    "        y = r * self.sind(lon)\n",
    "       \n",
    "        # Compute obliquity of ecliptic (inclination of Earth's axis)\n",
    "        obl_ecl = 23.4393 - 3.563E-7 * d\n",
    "       \n",
    "        # Convert to equatorial rectangular coordinates - x is unchanged\n",
    "        z = y * self.sind(obl_ecl)\n",
    "        y = y * self.cosd(obl_ecl)\n",
    " \n",
    "        # Convert to spherical coordinates\n",
    "        RA = self.atan2d(y, x)\n",
    "        dec = self.atan2d(z, math.sqrt(x*x + y*y))\n",
    " \n",
    "        return (RA, dec, r)\n",
    " \n",
    " \n",
    "    def revolution(self, x):\n",
    "        \"\"\"\n",
    "        This function reduces any angle to within the first revolution\n",
    "        by subtracting or adding even multiples of 360.0 until the    \n",
    "        result is >= 0.0 and < 360.0\n",
    "       \n",
    "        Reduce angle to within 0..360 degrees\n",
    "        \"\"\"\n",
    "        return (x - 360.0 * math.floor(x * self.INV360))\n",
    "   \n",
    "    def rev180(self, x):\n",
    "        \"\"\"\n",
    "        Reduce angle to within +180..+180 degrees\n",
    "        \"\"\"\n",
    "        return (x - 360.0 * math.floor(x * self.INV360 + 0.5))\n",
    " \n",
    "    def GMST0(self, d):\n",
    "        \"\"\"\n",
    "        This function computes GMST0, the Greenwich Mean Sidereal Time  \n",
    "        at 0h UT (i.e. the sidereal time at the Greenwhich meridian at  \n",
    "        0h UT).  GMST is then the sidereal time at Greenwich at any    \n",
    "        time of the day.  I've generalized GMST0 as well, and define it\n",
    "        as:  GMST0 = GMST - UT  --  this allows GMST0 to be computed at\n",
    "        other times than 0h UT as well.  While this sounds somewhat    \n",
    "        contradictory, it is very practical:  instead of computing      \n",
    "        GMST like:                                                      \n",
    "                                                                       \n",
    "         GMST = (GMST0) + UT * (366.2422/365.2422)                      \n",
    "                                                                       \n",
    "        where (GMST0) is the GMST last time UT was 0 hours, one simply  \n",
    "        computes:                                                      \n",
    "                                                                       \n",
    "         GMST = GMST0 + UT                                              \n",
    "                                                                       \n",
    "        where GMST0 is the GMST \"at 0h UT\" but at the current moment!  \n",
    "        Defined in this way, GMST0 will increase with about 4 min a    \n",
    "        day.  It also happens that GMST0 (in degrees, 1 hr = 15 degr)  \n",
    "        is equal to the Sun's mean longitude plus/minus 180 degrees!    \n",
    "        (if we neglect aberration, which amounts to 20 seconds of arc  \n",
    "        or 1.33 seconds of time)\n",
    "        \"\"\"\n",
    "        # Sidtime at 0h UT = L (Sun's mean longitude) + 180.0 degr  \n",
    "        # L = M + w, as defined in sunpos().  Since I'm too lazy to\n",
    "        # add these numbers, I'll let the C compiler do it for me.  \n",
    "        # Any decent C compiler will add the constants at compile  \n",
    "        # time, imposing no runtime or code overhead.              \n",
    "                                               \n",
    "        sidtim0 = self.revolution((180.0 + 356.0470 + 282.9404) +\n",
    "                                     (0.9856002585 + 4.70935E-5) * d)\n",
    "        return sidtim0;\n",
    " \n",
    "    def solar_altitude(self, latitude, year, month, day):\n",
    "        \"\"\"\n",
    "       Compute the altitude of the sun. No atmospherical refraction taken\n",
    "       in account.\n",
    "       Altitude of the southern hemisphere are given relative to\n",
    "       true north.\n",
    "       Altitude of the northern hemisphere are given relative to\n",
    "       true south.\n",
    "       Declination is between 23.5° North and 23.5° South depending\n",
    "       on the period of the year.\n",
    "       Source of formula for altitude is PhysicalGeography.net\n",
    "       http://www.physicalgeography.net/fundamentals/6h.html\n",
    "       \"\"\"\n",
    "        # Compute declination\n",
    "        N = self.daysSince2000Jan0(year, month, day)\n",
    "        res =  self.sunRADec(N)\n",
    "        declination = res[1]\n",
    "        sr = res[2]\n",
    " \n",
    "        # Compute the altitude\n",
    "        altitude = 90.0 - latitude  + declination\n",
    " \n",
    "        # In the tropical and  in extreme latitude, values over 90 may occurs.\n",
    "        if altitude > 90:\n",
    "            altitude = 90 - (altitude-90)\n",
    " \n",
    "        if altitude < 0:\n",
    "            altitude = 0\n",
    " \n",
    "        return altitude\n",
    " \n",
    "    def get_max_solar_flux(self, latitude, year, month, day):\n",
    "        \"\"\"\n",
    "       Compute the maximal solar flux to reach the ground for this date and\n",
    "       latitude.\n",
    "       Originaly comes from Environment Canada weather forecast model.\n",
    "       Information was of the public domain before release by Environment Canada\n",
    "       Output is in W/M^2.\n",
    "       \"\"\"\n",
    " \n",
    "        (fEot, fR0r, tDeclsc) = self.equation_of_time(year, month, day, latitude)\n",
    "        fSF = (tDeclsc[0]+tDeclsc[1])*fR0r\n",
    " \n",
    "        # In the case of a negative declinaison, solar flux is null\n",
    "        if fSF < 0:\n",
    "            fCoeff = 0\n",
    "        else:\n",
    "            fCoeff =  -1.56e-12*fSF**4 + 5.972e-9*fSF**3 -\\\n",
    "                     8.364e-6*fSF**2  + 5.183e-3*fSF - 0.435\n",
    "       \n",
    "        fSFT = fSF * fCoeff\n",
    " \n",
    "        if fSFT < 0:\n",
    "            fSFT=0\n",
    " \n",
    "        return fSFT\n",
    " \n",
    "    def equation_of_time(self, year, month, day, latitude):\n",
    "        \"\"\"\n",
    "       Description: Subroutine computing the part of the equation of time\n",
    "                    needed in the computing of the theoritical solar flux\n",
    "                    Correction originating of the CMC GEM model.\n",
    "                   \n",
    "       Parameters:  int nTime : cTime for the correction of the time.\n",
    " \n",
    "       Returns: tuple (double fEot, double fR0r, tuple tDeclsc)\n",
    "                dEot: Correction for the equation of time\n",
    "                dR0r: Corrected solar constant for the equation of time\n",
    "                tDeclsc: Declinaison\n",
    "       \"\"\"\n",
    "        # Julian date\n",
    "        nJulianDate = self.Julian(year, month, day)\n",
    "        # Check if it is a leap year\n",
    "        if(calendar.isleap(year)):\n",
    "            fDivide = 366.0\n",
    "        else:\n",
    "            fDivide = 365.0\n",
    "        # Correction for \"equation of time\"\n",
    "        fA = nJulianDate/fDivide*2*pi\n",
    "        fR0r = self.__Solcons(fA)*0.1367e4\n",
    "        fRdecl = 0.412*math.cos((nJulianDate+10.0)*2.0*pi/fDivide-pi)\n",
    "        fDeclsc1 = self.sind(latitude)*math.sin(fRdecl)\n",
    "        fDeclsc2 = self.cosd(latitude)*math.cos(fRdecl)\n",
    "        tDeclsc = (fDeclsc1, fDeclsc2)\n",
    "        # in minutes\n",
    "        fEot = 0.002733 -7.343*math.sin(fA)+ .5519*math.cos(fA) \\\n",
    "               - 9.47*math.sin(2.0*fA) - 3.02*math.cos(2.0*fA) \\\n",
    "               - 0.3289*math.sin(3.*fA) -0.07581*math.cos(3.0*fA) \\\n",
    "               -0.1935*math.sin(4.0*fA) -0.1245*math.cos(4.0*fA)\n",
    "        # Express in fraction of hour\n",
    "        fEot = fEot/60.0\n",
    "        # Express in radians\n",
    "        fEot = fEot*15*pi/180.0\n",
    " \n",
    "        return (fEot, fR0r, tDeclsc)\n",
    " \n",
    "    def __Solcons(self, dAlf):\n",
    "        \"\"\"\n",
    "       Name: __Solcons\n",
    "       \n",
    "       Parameters: [I] double dAlf : Solar constant to correct the excentricity\n",
    "       \n",
    "       Returns: double dVar : Variation of the solar constant\n",
    " \n",
    "       Functions Called: cos, sin\n",
    "       \n",
    "       Description:  Statement function that calculates the variation of the\n",
    "         solar constant as a function of the julian day. (dAlf, in radians)\n",
    "       \n",
    "       Notes: Comes from the\n",
    "       \n",
    "       Revision History:\n",
    "        Author          Date            Reason\n",
    "       Miguel Tremblay      June 30th 2004\n",
    "       \"\"\"\n",
    "       \n",
    "        dVar = 1.0/(1.0-9.464e-4*math.sin(dAlf)-0.01671*math.cos(dAlf)- \\\n",
    "                    + 1.489e-4*math.cos(2.0*dAlf)-2.917e-5*math.sin(3.0*dAlf)- \\\n",
    "                    + 3.438e-4*math.cos(4.0*dAlf))**2\n",
    "        return dVar\n",
    " \n",
    " \n",
    "    def Julian(self, year, month, day):\n",
    "        \"\"\"\n",
    "       Return julian day.\n",
    "       \"\"\"\n",
    "        if calendar.isleap(year): # Bissextil year, 366 days\n",
    "            lMonth = [0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335, 366]\n",
    "        else: # Normal year, 365 days\n",
    "            lMonth = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 365]\n",
    " \n",
    "        nJulian = lMonth[month-1] + day\n",
    "        return nJulian"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
